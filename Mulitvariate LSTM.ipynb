{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "\n",
    "EPOCHS = 500\n",
    "IN_SIZE = 5\n",
    "NUM_SAMPLES = 5\n",
    "\n",
    "def generate_data(rows, columns, samples):\n",
    "\tX = []\n",
    "\ty = []\n",
    "\ttransformations = {\n",
    "\t\t'11': lambda x, y: x + y,\n",
    "\t\t'15': lambda x, y: x - y,\n",
    "\t\t'10': lambda x, y: x * y,\n",
    "\t\t'30': lambda x, y: x / y,\n",
    "\t\t'2': lambda x, y: x + y,\n",
    "\t\t}\n",
    "\tfor j in range(samples):\n",
    "\t\tdata_set = []\n",
    "\t\tfor i in range(columns):\n",
    "\t\t\tdata = []\n",
    "\t\t\tfor val, fn in transformations.items():\n",
    "\t\t\t\tdata.append(int(fn(int(val), i+j+1)))\n",
    "\t\t\tdata_set.append(data)\n",
    "\t\tX.append(data_set)\n",
    "\t\ty.append([j+1])\n",
    "\treturn X, y\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = generate_data(IN_SIZE, 3, NUM_SAMPLES)\n",
    "X = torch.FloatTensor(X)\n",
    "y = torch.LongTensor(y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 3, 5])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "\tdef __init__(self):\n",
    "\t\tsuper(RNN, self).__init__()\n",
    "\n",
    "self.rnn = nn.LSTM(\n",
    "input_size=5,\n",
    "hidden_size=NUM_SAMPLES+1,\n",
    "num_layers=2,\n",
    "batch_first=True,\n",
    "    )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out, (h_n, h_c) = self.rnn(x, None)\n",
    "        return out[:, -1, :]\t# Return output at last time-step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:  1.7038245\n",
      "Loss:  1.6977987\n",
      "Loss:  1.6912194\n",
      "Loss:  1.6852239\n",
      "Loss:  1.6797185\n",
      "Loss:  1.6746771\n",
      "Loss:  1.670292\n",
      "Loss:  1.6661247\n",
      "Loss:  1.661568\n",
      "Loss:  1.6578254\n",
      "Loss:  1.6551144\n",
      "Loss:  1.6526159\n",
      "Loss:  1.6503209\n",
      "Loss:  1.6504378\n",
      "Loss:  1.6531541\n",
      "Loss:  1.6509528\n",
      "Loss:  1.6471616\n",
      "Loss:  1.6417133\n",
      "Loss:  1.6332117\n",
      "Loss:  1.6224278\n",
      "Loss:  1.6100878\n",
      "Loss:  1.5963761\n",
      "Loss:  1.5813938\n",
      "Loss:  1.5652723\n",
      "Loss:  1.5481452\n",
      "Loss:  1.53014\n",
      "Loss:  1.5113792\n",
      "Loss:  1.4919777\n",
      "Loss:  1.4720434\n",
      "Loss:  1.4516757\n",
      "Loss:  1.4309648\n",
      "Loss:  1.409991\n",
      "Loss:  1.3888255\n",
      "Loss:  1.3675321\n",
      "Loss:  1.3461736\n",
      "Loss:  1.3248203\n",
      "Loss:  1.3035632\n",
      "Loss:  1.2825264\n",
      "Loss:  1.2618731\n",
      "Loss:  1.2418096\n",
      "Loss:  1.2225635\n",
      "Loss:  1.204271\n",
      "Loss:  1.1870277\n",
      "Loss:  1.1709806\n",
      "Loss:  1.1562387\n",
      "Loss:  1.1428763\n",
      "Loss:  1.1309279\n",
      "Loss:  1.1204059\n",
      "Loss:  1.1113287\n",
      "Loss:  1.1037747\n",
      "Loss:  1.0979922\n",
      "Loss:  1.094652\n",
      "Loss:  1.0953122\n",
      "Loss:  1.1017231\n",
      "Loss:  1.1087108\n",
      "Loss:  1.1078317\n",
      "Loss:  1.1036261\n",
      "Loss:  1.099359\n",
      "Loss:  1.0949875\n",
      "Loss:  1.0904562\n",
      "Loss:  1.0858054\n",
      "Loss:  1.0810719\n",
      "Loss:  1.0762724\n",
      "Loss:  1.0714228\n",
      "Loss:  1.0665395\n",
      "Loss:  1.0616404\n",
      "Loss:  1.056742\n",
      "Loss:  1.051861\n",
      "Loss:  1.0470116\n",
      "Loss:  1.0422065\n",
      "Loss:  1.0374562\n",
      "Loss:  1.0327685\n",
      "Loss:  1.0281494\n",
      "Loss:  1.0236019\n",
      "Loss:  1.0191284\n",
      "Loss:  1.0147288\n",
      "Loss:  1.0104018\n",
      "Loss:  1.0061452\n",
      "Loss:  1.0019553\n",
      "Loss:  0.997828\n",
      "Loss:  0.9937588\n",
      "Loss:  0.9897422\n",
      "Loss:  0.9857728\n",
      "Loss:  0.98184496\n",
      "Loss:  0.97795296\n",
      "Loss:  0.97409075\n",
      "Loss:  0.9702527\n",
      "Loss:  0.96643347\n",
      "Loss:  0.9626274\n",
      "Loss:  0.95883\n",
      "Loss:  0.95503765\n",
      "Loss:  0.9512575\n",
      "Loss:  0.94733953\n",
      "Loss:  0.94342774\n",
      "Loss:  0.9395059\n",
      "Loss:  0.9355896\n",
      "Loss:  0.9316781\n",
      "Loss:  0.9277487\n",
      "Loss:  0.9237971\n",
      "Loss:  0.9198262\n",
      "Testing:\n",
      "========\n",
      "tensor([1])\n",
      "tensor(1)\n",
      "tensor([2])\n",
      "tensor(2)\n",
      "tensor([3])\n",
      "tensor(3)\n",
      "tensor([4])\n",
      "tensor(4)\n",
      "tensor([5])\n",
      "tensor(5)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "X, y = generate_data(IN_SIZE, 2, NUM_SAMPLES)\n",
    "X = torch.FloatTensor(X)\n",
    "y = torch.LongTensor(y)\n",
    "\n",
    "rnn = RNN()\n",
    "optimizer = torch.optim.Adam(rnn.parameters(), lr=0.001)\n",
    "loss_func = nn.CrossEntropyLoss()     \n",
    "\n",
    "\n",
    "for j in range(EPOCHS):\n",
    "\tfor i, item in enumerate(X):\n",
    "\t\titem = item.unsqueeze(0)\n",
    "\t\toutput = rnn(item)\n",
    "\t\tloss = loss_func(output, y[i])\n",
    "\t\toptimizer.zero_grad()\n",
    "\t\tloss.backward()\n",
    "\t\toptimizer.step()\n",
    "\n",
    "\tif j % 5 == 0:\n",
    "\t\tprint('Loss: ', np.average(loss.detach()))\n",
    "\n",
    "print('Testing:\\n========')\n",
    "for i, item in enumerate(X):\n",
    "\tprint(y[i])\n",
    "\toutp = rnn(item.unsqueeze(0))\n",
    "\tprint(np.argmax(outp.detach()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 5])\n",
      "torch.Size([1, 2, 5])\n"
     ]
    }
   ],
   "source": [
    "print(item.shape)\n",
    "print(item.unsqueeze(0).shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multivariate many to many"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "4\n",
    "I would like to implement LSTM for multivariate input in Pytorch.\n",
    "\n",
    "Following this article https://machinelearningmastery.com/how-to-develop-lstm-models-for-time-series-forecasting/ which uses keras, the input data are in shape of (number of samples, number of timesteps, number of parallel features)\n",
    "\n",
    "in_seq1 = array([10, 20, 30, 40, 50, 60, 70, 80, 90])\n",
    "in_seq2 = array([15, 25, 35, 45, 55, 65, 75, 85, 95])\n",
    "out_seq = array([in_seq1[i]+in_seq2[i] for i in range(len(in_seq1))])\n",
    ". . . \n",
    "Input     Output\n",
    "[[10 15]\n",
    " [20 25]\n",
    " [30 35]] 65\n",
    "[[20 25]\n",
    " [30 35]\n",
    " [40 45]] 85\n",
    "[[30 35]\n",
    " [40 45]\n",
    " [50 55]] 105\n",
    "[[40 45]\n",
    " [50 55]\n",
    " [60 65]] 125\n",
    "[[50 55]\n",
    " [60 65]\n",
    " [70 75]] 145\n",
    "[[60 65]\n",
    " [70 75]\n",
    " [80 85]] 165\n",
    "[[70 75]\n",
    " [80 85]\n",
    " [90 95]] 185\n",
    "\n",
    "n_timesteps = 3\n",
    "n_features = 2\n",
    "In keras it seems to be easy:\n",
    "\n",
    "model.add(LSTM(50, activation='relu', input_shape=(n_timesteps, n_features)))\n",
    "\n",
    "Can it be done in other way, than creating n_features of LSTMs as first layer and feed each separately (imagine as multiple streams of sequences) and then flatten their output to linear layer?\n",
    "\n",
    "I'm not 100% sure but by nature of LSTM the input cannot be flattened and passed as 1D array, because each sequence \"plays by different rules\" which the LSTM is supposed to learn.\n",
    "\n",
    "So how does such implementation with keras equal to PyTorch input of shape (seq_len, batch, input_size)(source https://pytorch.org/docs/stable/nn.html#lstm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# multivariate data preparation\n",
    "from numpy import array\n",
    "from numpy import hstack\n",
    " \n",
    "# split a multivariate sequence into samples\n",
    "def split_sequences(sequences, n_steps):\n",
    "    X, y = list(), list()\n",
    "    for i in range(len(sequences)):\n",
    "        # find the end of this pattern\n",
    "        end_ix = i + n_steps\n",
    "        # check if we are beyond the dataset\n",
    "        if end_ix > len(sequences):\n",
    "            break\n",
    "        # gather input and output parts of the pattern\n",
    "        seq_x, seq_y = sequences[i:end_ix, :-1], sequences[end_ix-1, -1]\n",
    "        X.append(seq_x)\n",
    "        y.append(seq_y)\n",
    "    return array(X), array(y)\n",
    " \n",
    "# define input sequence\n",
    "in_seq1 = array([x for x in range(0,100,10)])\n",
    "in_seq2 = array([x for x in range(5,105,10)])\n",
    "out_seq = array([in_seq1[i]+in_seq2[i] for i in range(len(in_seq1))])\n",
    "# convert to [rows, columns] structure\n",
    "in_seq1 = in_seq1.reshape((len(in_seq1), 1))\n",
    "in_seq2 = in_seq2.reshape((len(in_seq2), 1))\n",
    "out_seq = out_seq.reshape((len(out_seq), 1))\n",
    "# horizontally stack columns\n",
    "dataset = hstack((in_seq1, in_seq2, out_seq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 1)\n",
      "(10, 1)\n",
      "(10, 1)\n"
     ]
    }
   ],
   "source": [
    "print(in_seq1.shape)\n",
    "print(in_seq2.shape)\n",
    "print(out_seq.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MV_LSTM(torch.nn.Module):\n",
    "    def __init__(self,n_features,seq_length,output_values=1):\n",
    "        super(MV_LSTM, self).__init__()\n",
    "        self.n_features = n_features\n",
    "        self.seq_len = seq_length\n",
    "        self.n_hidden = 20 # number of hidden states\n",
    "        self.n_layers = 1 # number of LSTM layers (stacked)\n",
    "        self.output_values=output_values\n",
    "    \n",
    "        self.l_lstm = torch.nn.LSTM(input_size = n_features, \n",
    "                                 hidden_size = self.n_hidden,\n",
    "                                 num_layers = self.n_layers, \n",
    "                                 batch_first = True)\n",
    "        # according to pytorch docs LSTM output is \n",
    "        # (batch_size,seq_len, num_directions * hidden_size)\n",
    "        # when considering batch_first = True\n",
    "        self.l_linear = torch.nn.Linear(self.n_hidden*self.seq_len, output_values)\n",
    "        \n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        # even with batch_first = True this remains same as docs\n",
    "        hidden_state = torch.zeros(self.n_layers,batch_size,self.n_hidden)\n",
    "        cell_state = torch.zeros(self.n_layers,batch_size,self.n_hidden)\n",
    "        self.hidden = (hidden_state, cell_state)\n",
    "    \n",
    "    \n",
    "    def forward(self, x):        \n",
    "        batch_size, seq_len, _ = x.size()\n",
    "        \n",
    "        lstm_out, self.hidden = self.l_lstm(x,self.hidden)\n",
    "        # lstm_out(with batch_first = True) is \n",
    "        # (batch_size,seq_len,num_directions * hidden_size)\n",
    "        # for following linear layer we want to keep batch_size dimension and merge rest       \n",
    "        # .contiguous() -> solves tensor compatibility error\n",
    "        x = lstm_out.contiguous().view(batch_size,-1)\n",
    "        print(x.shape)\n",
    "        out=self.l_linear(x)\n",
    "        #return self.l_linear(x)\n",
    "        return x,out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7, 4, 2) (7,)\n"
     ]
    }
   ],
   "source": [
    "n_features = 2 # this is number of parallel inputs\n",
    "n_timesteps = 4 # this is number of timesteps\n",
    "\n",
    "# convert dataset into input/output\n",
    "X, y = split_sequences(dataset, n_timesteps)\n",
    "print(X.shape, y.shape)\n",
    "\n",
    "# create NN\n",
    "mv_net = MV_LSTM(n_features,n_timesteps)\n",
    "criterion = torch.nn.MSELoss() # reduction='sum' created huge loss value\n",
    "optimizer = torch.optim.Adam(mv_net.parameters(), lr=1e-1)\n",
    "\n",
    "train_episodes = 500\n",
    "batch_size = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 463,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 45,  65,  85, 105, 125, 145, 165, 185])"
      ]
     },
     "execution_count": 463,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.1889, -0.2185,  0.0583, -0.1242,  0.0382, -0.4744, -0.2287,  0.1097,\n",
       "        -0.0973,  0.0178, -0.0937, -0.0162, -0.1122, -0.0310, -0.1566, -0.1122,\n",
       "        -0.0310, -0.1566, -0.1122, -0.0310, -0.1566, -0.1122, -0.0310, -0.1566],\n",
       "       grad_fn=<ViewBackward>)"
      ]
     },
     "execution_count": 295,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_model.view(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step :  0 loss :  0.005418463610112667\n",
      "step :  50 loss :  0.0038068764843046665\n",
      "step :  100 loss :  0.0026473659090697765\n",
      "step :  150 loss :  0.001821596291847527\n",
      "step :  200 loss :  0.0012401746353134513\n",
      "step :  250 loss :  0.0008350973948836327\n",
      "step :  300 loss :  0.0005560870631597936\n",
      "step :  350 loss :  0.00036595095298253\n",
      "step :  400 loss :  0.00023786402016412467\n",
      "step :  450 loss :  0.0001529715082142502\n"
     ]
    }
   ],
   "source": [
    "mv_net.train()\n",
    "for t in range(train_episodes):\n",
    "    for b in range(0,len(X),batch_size):\n",
    "        optimizer.zero_grad()\n",
    "        inpt = X[b:b+batch_size,:,:]\n",
    "        target = y[b:b+batch_size]    \n",
    "        \n",
    "        x_batch = torch.tensor(inpt,dtype=torch.float32)    \n",
    "        y_batch = torch.tensor(target,dtype=torch.float32)\n",
    "    \n",
    "        mv_net.init_hidden(x_batch.size(0))\n",
    "    #    lstm_out, _ = mv_net.l_lstm(x_batch,nnet.hidden)    \n",
    "    #    lstm_out.contiguous().view(x_batch.size(0),-1)\n",
    "        output = mv_net(x_batch) \n",
    "        loss = criterion(output.view(-1), y_batch)  \n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()        \n",
    "        optimizer.zero_grad()\n",
    "    if t%50==0:\n",
    "        print('step : ' , t , 'loss : ' , loss.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 615,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_size' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-615-363acb6900a3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0my_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Creating a data structure with n timesteps which is 10 in our example\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_size\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtimesteps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimesteps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_size\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtimesteps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mX_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_set\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mtimesteps\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_size' is not defined"
     ]
    }
   ],
   "source": [
    "X_train = []\n",
    "y_train = []\n",
    "# Creating a data structure with n timesteps which is 10 in our example\n",
    "print(train_size + timesteps)\n",
    "for i in range(timesteps, train_size + timesteps): \n",
    "    X_train.append(X_train_set[i-timesteps:i,:])\n",
    "    y_train.append(y_train_set[i:i+timesteps])\n",
    "print('len(X_train)',len(X_train))\n",
    "print('len(y_train)',len(y_train))\n",
    "#create X_train matrix\n",
    "#10 items per array (timestep) \n",
    "print('np.array(X_train).shape',np.array(X_train).shape)\n",
    "print('X_train[1].shap',X_train[1].shape)\n",
    "#create Y_train matrix\n",
    "#10 items per array (as much as timestep) \n",
    "print('y_train[0:2]',y_train[0:2])\n",
    "print('np.array(y_train).shape',np.array(y_train).shape)\n",
    "print('############################')\n",
    "print('############################')\n",
    "print('############################')\n",
    "# Do the same for your test data\n",
    "X_test = []\n",
    "y_test = []\n",
    "# Creating a data structure with n timesteps\n",
    "print(test_size + timesteps)\n",
    "for i in range(timesteps, test_size + timesteps): \n",
    "    X_test.append(X_test_set[i-timesteps:i,:])\n",
    "    y_test.append(y_test_set[i:i+timesteps])\n",
    "print('len(X_test)',len(X_test))\n",
    "print('len(y_test)',len(y_test))\n",
    "#create X_test matrix\n",
    "#10 items per array (timestep) \n",
    "print('np.array(X_test).shape',np.array(X_test).shape)\n",
    "print('X_test[1].shape',X_test[1].shape)\n",
    "#10 items per array (timestep) \n",
    "print('y_test[0:2]',y_test[0:2])\n",
    "print('np.array(y_test).shape',np.array(y_test).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# trying it in the GL example\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_halo=pd.read_csv('halo_vs_non_halo_ops_ccp.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>store_id</th>\n",
       "      <th>first_date</th>\n",
       "      <th>gl</th>\n",
       "      <th>Direct_OPS</th>\n",
       "      <th>Halo_OPS</th>\n",
       "      <th>total_ops</th>\n",
       "      <th>ops</th>\n",
       "      <th>net_ccp</th>\n",
       "      <th>gross_cplf</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7153661</th>\n",
       "      <td>yearmirr07206-20</td>\n",
       "      <td>2019-11-01</td>\n",
       "      <td>641.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>26.50</td>\n",
       "      <td>26.50</td>\n",
       "      <td>26.50</td>\n",
       "      <td>6.183704</td>\n",
       "      <td>13.08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3998051</th>\n",
       "      <td>alomil0f-20</td>\n",
       "      <td>2020-02-01</td>\n",
       "      <td>23.0</td>\n",
       "      <td>31.26</td>\n",
       "      <td>77.77</td>\n",
       "      <td>109.03</td>\n",
       "      <td>113.22</td>\n",
       "      <td>30.696362</td>\n",
       "      <td>11.27</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 store_id  first_date     gl  Direct_OPS  Halo_OPS  total_ops  \\\n",
       "7153661  yearmirr07206-20  2019-11-01  641.0        0.00     26.50      26.50   \n",
       "3998051       alomil0f-20  2020-02-01   23.0       31.26     77.77     109.03   \n",
       "\n",
       "            ops    net_ccp  gross_cplf  \n",
       "7153661   26.50   6.183704       13.08  \n",
       "3998051  113.22  30.696362       11.27  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_halo.sample(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_halo=pd.read_csv('halo_vs_non_halo_ops_ccp.csv')\n",
    "df_sample=df_halo.merge(df_halo[['store_id']].drop_duplicates().sample(5000),\n",
    "                       on='store_id',\n",
    "                       how='inner')\n",
    "del df_halo\n",
    "gl_top_30=[147,201,121,200,79,21,193,194,60,14,107,23,421,263,86,468,469,199,309,229,196,325,328,267,75,63,351,504,198,228]\n",
    "df_sample['gl_top_30']=np.where(df_sample['gl'].isin(gl_top_30[:4]),\n",
    "                                df_sample['gl'],\n",
    "                                '-999'\n",
    "                               )\n",
    "columns=list(set(df_sample['gl_top_30'].values))\n",
    "\n",
    "df_sample[['Direct_OPS','Halo_OPS','total_ops','ops','net_ccp','gross_cplf']]=df_sample[['Direct_OPS','Halo_OPS','total_ops','ops','net_ccp','gross_cplf']].fillna(0)\n",
    "df_sample['first_date']=pd.to_datetime(df_sample['first_date'])\n",
    "df_sample_1=df_sample.groupby(['store_id','first_date','gl_top_30']).agg({'Direct_OPS':sum,\n",
    "                                                             'Halo_OPS':sum,\n",
    "                                                             'total_ops':sum,\n",
    "                                                             'net_ccp':sum}).reset_index()\n",
    "df_sample_1=df_sample_1.set_index(['store_id','first_date','gl_top_30']).unstack(level=[2,1]).fillna(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sample=df_halo.merge(df_halo[['store_id']].drop_duplicates().sample(3000),\n",
    "                       on='store_id',\n",
    "                       how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(59182, 9)\n",
      "3000\n"
     ]
    }
   ],
   "source": [
    "print(df_sample.shape)\n",
    "print(df_sample.store_id.nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "del df_halo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "gl_top_30=[147,201,121,200,79,21,193,194,60,14,107,23,421,263,86,468,469,199,309,229,196,325,328,267,75,63,351,504,198,228]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_sample['gl_top_30']=np.where(df_sample['gl'].isin(gl_top_30[:2]),\n",
    "                                df_sample['gl'],\n",
    "                                '-999'\n",
    "                               )\n",
    "columns=list(set(df_sample['gl_top_30'].values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sample[['Direct_OPS','Halo_OPS','total_ops','ops','net_ccp','gross_cplf']]=df_sample[['Direct_OPS','Halo_OPS','total_ops','ops','net_ccp','gross_cplf']].fillna(0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sample['first_date']=pd.to_datetime(df_sample['first_date'])\n",
    "df_sample_1=df_sample.groupby(['store_id','first_date','gl_top_30']).agg({'Direct_OPS':sum,\n",
    "                                                             'Halo_OPS':sum,\n",
    "                                                             'total_ops':sum,\n",
    "                                                             'net_ccp':sum}).reset_index()\n",
    "df_sample_1=df_sample_1.set_index(['store_id','first_date','gl_top_30']).unstack(level=[2,1]).fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_frame=2\n",
    "start_frame=1\n",
    "total_time_stamps=df_sample_1['total_ops'].shape[1]\n",
    "target_stamp_start=start_frame+seq_frame\n",
    "time_steps=np.arange(target_stamp_start,total_time_stamps,1)\n",
    "features="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X=[]\n",
    "y=[]\n",
    "for i in time_steps:\n",
    "    feature_end=i-1\n",
    "    feature_start=feature_end-seq_frame\n",
    "    features=df_sample_1['total_ops'].iloc[:,feature_start:feature_end].values\n",
    "    target=df_sample_1['total_ops'].iloc[:,i].values\n",
    "    X.append(features)\n",
    "    y.append(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 552,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_array_1=[]\n",
    "target_array_1=[]\n",
    "\n",
    "seq_frame=3\n",
    "start_frame=1\n",
    "#total_time_stamps=df_sample_1['total_ops'].shape[1]\n",
    "#target_stamp_start=start_frame+seq_frame\n",
    "#time_steps=np.arange(target_stamp_start,total_time_stamps,1)\n",
    "feature='Direct_OPS'\n",
    "feature_1='Direct_OPS'\n",
    "month_values=pd.Series(sorted(set(df_sample_1.columns.get_level_values(2))))\n",
    "#gls=pd.Series(sorted(set(df_sample_1.columns.get_level_values(1))))\n",
    "\n",
    "total_time_stamps=len(month_values)\n",
    "time_steps=np.arange(total_time_stamps)\n",
    "target_stamp_start=seq_frame+1\n",
    "target_time_steps=np.arange(target_stamp_start,total_time_stamps)\n",
    "i=0\n",
    "target_month=month_values[target_time_steps[i]]\n",
    "feature_months=month_values[target_time_steps[i]-2:target_time_steps[i]]\n",
    "target=df_sample_1.loc[ :,(feature,slice(None),target_month)]\n",
    "features=df_sample_1.loc[ :,(feature_1,slice(None),feature_months)][feature_1]\n",
    "gls=list(set(features.columns.get_level_values(0)))\n",
    "target_values=target.values\n",
    "#feature_list_1=features[gls[0]].values\n",
    "#feature_list_2=features[gls[1]].values\n",
    "feature_list=[]\n",
    "for gl in gls:\n",
    "    feature_list.append(features[gl].values)\n",
    "feature_array=array(feature_list)\n",
    "feature_array=np.transpose(feature_array,(1,0,2))\n",
    "\n",
    "feature_array_1.append(feature_array)\n",
    "target_array_1.append(target_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sample_2=df_sample_1.swaplevel(0,2,axis=1).swaplevel(1,2,axis=1).stack(0)\n",
    "feature=df_sample_2.swaplevel(1,0,axis=0)[['Direct_OPS','Halo_OPS']]\n",
    "feature.columns=feature.columns.map('_'.join).str.strip('|')\n",
    "feature_names=feature.columns\n",
    "target_names=df_sample_2.swaplevel(1,0,axis=0)[['Direct_OPS']].columns.map('_'.join).str.strip('|')\n",
    "feature_df=feature.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "#store_feature=feature_df.query('store_id==\"techupdate09-20\"')\n",
    "store_feature=feature_df.query('store_id==\"techupdate09-20\"|store_id==\"thestegui09-20\"')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>first_date</th>\n",
       "      <th>store_id</th>\n",
       "      <th>Direct_OPS_-999</th>\n",
       "      <th>Direct_OPS_147.0</th>\n",
       "      <th>Direct_OPS_201.0</th>\n",
       "      <th>Halo_OPS_-999</th>\n",
       "      <th>Halo_OPS_147.0</th>\n",
       "      <th>Halo_OPS_201.0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>15216</th>\n",
       "      <td>2019-10-01</td>\n",
       "      <td>techupdate09-20</td>\n",
       "      <td>13736.42</td>\n",
       "      <td>83.68</td>\n",
       "      <td>59.00</td>\n",
       "      <td>15153.24</td>\n",
       "      <td>1586.49</td>\n",
       "      <td>536.85</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15217</th>\n",
       "      <td>2019-11-01</td>\n",
       "      <td>techupdate09-20</td>\n",
       "      <td>14065.72</td>\n",
       "      <td>78.35</td>\n",
       "      <td>125.92</td>\n",
       "      <td>16293.61</td>\n",
       "      <td>969.70</td>\n",
       "      <td>909.51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15218</th>\n",
       "      <td>2019-12-01</td>\n",
       "      <td>techupdate09-20</td>\n",
       "      <td>21281.44</td>\n",
       "      <td>103.64</td>\n",
       "      <td>945.89</td>\n",
       "      <td>21856.48</td>\n",
       "      <td>1334.33</td>\n",
       "      <td>1703.18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15219</th>\n",
       "      <td>2020-01-01</td>\n",
       "      <td>techupdate09-20</td>\n",
       "      <td>13490.32</td>\n",
       "      <td>64.60</td>\n",
       "      <td>380.72</td>\n",
       "      <td>14350.87</td>\n",
       "      <td>1044.80</td>\n",
       "      <td>836.72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15220</th>\n",
       "      <td>2020-02-01</td>\n",
       "      <td>techupdate09-20</td>\n",
       "      <td>10989.36</td>\n",
       "      <td>38.85</td>\n",
       "      <td>224.93</td>\n",
       "      <td>13237.01</td>\n",
       "      <td>678.85</td>\n",
       "      <td>394.82</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15221</th>\n",
       "      <td>2020-03-01</td>\n",
       "      <td>techupdate09-20</td>\n",
       "      <td>8791.04</td>\n",
       "      <td>0.00</td>\n",
       "      <td>469.74</td>\n",
       "      <td>9618.83</td>\n",
       "      <td>1205.35</td>\n",
       "      <td>548.53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15954</th>\n",
       "      <td>2019-10-01</td>\n",
       "      <td>thestegui09-20</td>\n",
       "      <td>64.81</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>101.94</td>\n",
       "      <td>0.00</td>\n",
       "      <td>44.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15955</th>\n",
       "      <td>2019-11-01</td>\n",
       "      <td>thestegui09-20</td>\n",
       "      <td>49.41</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>52.22</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15956</th>\n",
       "      <td>2019-12-01</td>\n",
       "      <td>thestegui09-20</td>\n",
       "      <td>77.02</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>33.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15957</th>\n",
       "      <td>2020-01-01</td>\n",
       "      <td>thestegui09-20</td>\n",
       "      <td>136.56</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>186.43</td>\n",
       "      <td>0.00</td>\n",
       "      <td>9.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15958</th>\n",
       "      <td>2020-02-01</td>\n",
       "      <td>thestegui09-20</td>\n",
       "      <td>8.38</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>208.67</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15959</th>\n",
       "      <td>2020-03-01</td>\n",
       "      <td>thestegui09-20</td>\n",
       "      <td>11.96</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>297.16</td>\n",
       "      <td>0.00</td>\n",
       "      <td>3.28</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      first_date         store_id  Direct_OPS_-999  Direct_OPS_147.0  \\\n",
       "15216 2019-10-01  techupdate09-20         13736.42             83.68   \n",
       "15217 2019-11-01  techupdate09-20         14065.72             78.35   \n",
       "15218 2019-12-01  techupdate09-20         21281.44            103.64   \n",
       "15219 2020-01-01  techupdate09-20         13490.32             64.60   \n",
       "15220 2020-02-01  techupdate09-20         10989.36             38.85   \n",
       "15221 2020-03-01  techupdate09-20          8791.04              0.00   \n",
       "15954 2019-10-01   thestegui09-20            64.81              0.00   \n",
       "15955 2019-11-01   thestegui09-20            49.41              0.00   \n",
       "15956 2019-12-01   thestegui09-20            77.02              0.00   \n",
       "15957 2020-01-01   thestegui09-20           136.56              0.00   \n",
       "15958 2020-02-01   thestegui09-20             8.38              0.00   \n",
       "15959 2020-03-01   thestegui09-20            11.96              0.00   \n",
       "\n",
       "       Direct_OPS_201.0  Halo_OPS_-999  Halo_OPS_147.0  Halo_OPS_201.0  \n",
       "15216             59.00       15153.24         1586.49          536.85  \n",
       "15217            125.92       16293.61          969.70          909.51  \n",
       "15218            945.89       21856.48         1334.33         1703.18  \n",
       "15219            380.72       14350.87         1044.80          836.72  \n",
       "15220            224.93       13237.01          678.85          394.82  \n",
       "15221            469.74        9618.83         1205.35          548.53  \n",
       "15954              0.00         101.94            0.00           44.99  \n",
       "15955              0.00          52.22            0.00            0.00  \n",
       "15956              0.00           0.00            0.00           33.03  \n",
       "15957              0.00         186.43            0.00            9.99  \n",
       "15958              0.00         208.67            0.00            0.00  \n",
       "15959              0.00         297.16            0.00            3.28  "
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "store_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 646,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Timestamp('2019-10-01 00:00:00')"
      ]
     },
     "execution_count": 646,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_df=feature.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "start_index=month_seris[starting_numeric_index]\n",
    "\n",
    "#step_length=(start_index-end_index)/(len(month_seris)-len_time_steps)\n",
    "#for i in np.arange(start_index,end_index,):\n",
    "    #indices=np.arange(start_index)\n",
    "    #print(i)\n",
    "    #print(store_feature[store_feature[column_to_index]==i])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i+len_time_steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Timestamp('2020-01-01 00:00:00')"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "month_seris[i+len_time_steps+1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Timestamp('2020-01-01 00:00:00')"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "month_seris[i+len_time_steps+1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>first_date</th>\n",
       "      <th>store_id</th>\n",
       "      <th>Direct_OPS_-999</th>\n",
       "      <th>Direct_OPS_147.0</th>\n",
       "      <th>Direct_OPS_201.0</th>\n",
       "      <th>Halo_OPS_-999</th>\n",
       "      <th>Halo_OPS_147.0</th>\n",
       "      <th>Halo_OPS_201.0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>15219</th>\n",
       "      <td>2020-01-01</td>\n",
       "      <td>techupdate09-20</td>\n",
       "      <td>13490.32</td>\n",
       "      <td>64.6</td>\n",
       "      <td>380.72</td>\n",
       "      <td>14350.87</td>\n",
       "      <td>1044.8</td>\n",
       "      <td>836.72</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      first_date         store_id  Direct_OPS_-999  Direct_OPS_147.0  \\\n",
       "15219 2020-01-01  techupdate09-20         13490.32              64.6   \n",
       "\n",
       "       Direct_OPS_201.0  Halo_OPS_-999  Halo_OPS_147.0  Halo_OPS_201.0  \n",
       "15219            380.72       14350.87          1044.8          836.72  "
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "store_feature[store_feature[column_to_index]==month_seris[i+len_time_steps+1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [],
   "source": [
    "#store_feature_1=feature_df.query('store_id==\"techupdate09-20\"|store_id==\"thestegui09-20\"')\n",
    "store_feature_1=feature_df#.query('store_id==\"techupdate09-20\"|store_id==\"thestegui09-20\"')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "thestegui09-20\n",
      "techupdate09-20\n"
     ]
    }
   ],
   "source": [
    "for store in set(store_feature.store_id):\n",
    "    print(store)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_to_index='first_date'\n",
    "month_seris=pd.Series(sorted(set(feature_df[column_to_index])))\n",
    "len_time_steps=2\n",
    "starting_numeric_index=0\n",
    "ending_numeric_index=len(month_seris)-len_time_steps\n",
    "non_features=[i for i in feature_df.columns if i not in feature_names]\n",
    "non_features_for_modelling=[]\n",
    "features_for_modelling=[]\n",
    "target_for_modelling=[]\n",
    "for store in set(store_feature_1.store_id):\n",
    "    store_feature=store_feature_1[store_feature_1['store_id']==store]\n",
    "    for i in np.arange(starting_numeric_index,ending_numeric_index):\n",
    "        #print(i)\n",
    "        #print(month_seris[i:i+len_time_steps])\n",
    "        store_feature_for_the_iteration=store_feature[store_feature[column_to_index].isin(month_seris[i:i+len_time_steps])][feature_names]\n",
    "        non_store_feature_for_the_iteration=store_feature[store_feature[column_to_index].isin(month_seris[i:i+len_time_steps])][non_features]\n",
    "        target_feature_for_the_iteration=store_feature[store_feature[column_to_index]==month_seris[i+len_time_steps]][target_names]\n",
    "        features_for_modelling.append(store_feature_for_the_iteration.values)\n",
    "        non_features_for_modelling.append(non_store_feature_for_the_iteration.values)\n",
    "        target_for_modelling.append(target_feature_for_the_iteration.values)\n",
    "\n",
    "model_X=np.array(features_for_modelling)\n",
    "target_Y=np.array(target_for_modelling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12000, 2, 6)\n",
      "(12000, 1, 3)\n"
     ]
    }
   ],
   "source": [
    "print(model_X.shape)\n",
    "print(target_Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 524,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 528,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MV_LSTM(torch.nn.Module):\n",
    "    def __init__(self,n_features,seq_length,output_values=1):\n",
    "        super(MV_LSTM, self).__init__()\n",
    "        self.n_features = n_features\n",
    "        self.seq_len = seq_length\n",
    "        self.n_hidden = 20 # number of hidden states\n",
    "        self.n_layers = 1 # number of LSTM layers (stacked)\n",
    "        self.output_values=output_values\n",
    "    \n",
    "        self.l_lstm = torch.nn.LSTM(input_size = n_features, \n",
    "                                 hidden_size = self.n_hidden,\n",
    "                                 num_layers = self.n_layers, \n",
    "                                 batch_first = True)\n",
    "        # according to pytorch docs LSTM output is \n",
    "        # (batch_size,seq_len, num_directions * hidden_size)\n",
    "        # when considering batch_first = True\n",
    "        #print(self.n_hidden)\n",
    "        #print(self.seq_len)\n",
    "        #print(seq_length)\n",
    "        #print(self.n_hidden*self.seq_len)\n",
    "        \n",
    "        self.l_linear = torch.nn.Linear(self.n_hidden*self.seq_len, output_values)\n",
    "        \n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        # even with batch_first = True this remains same as docs\n",
    "        hidden_state = torch.zeros(self.n_layers,batch_size,self.n_hidden)\n",
    "        cell_state = torch.zeros(self.n_layers,batch_size,self.n_hidden)\n",
    "        self.hidden = (hidden_state, cell_state)\n",
    "    \n",
    "    \n",
    "    def forward(self, x):        \n",
    "        batch_size, seq_len, _ = x.size()\n",
    "        \n",
    "        lstm_out, self.hidden = self.l_lstm(x,self.hidden)\n",
    "        # lstm_out(with batch_first = True) is \n",
    "        # (batch_size,seq_len,num_directions * hidden_size)\n",
    "        # for following linear layer we want to keep batch_size dimension and merge rest       \n",
    "        # .contiguous() -> solves tensor compatibility error\n",
    "        x = lstm_out.contiguous().view(batch_size,-1)\n",
    "        #print(x.shape)\n",
    "        #out=self.l_linear(x)\n",
    "        x=F.relu(x)\n",
    "        return self.l_linear(x)\n",
    "        #return x,out\n",
    "    \n",
    "    \n",
    "n_features=model_X.shape[2]\n",
    "n_timestep=model_X.shape[1]\n",
    "output_values=target_Y.shape[2]\n",
    "mv_net = MV_LSTM(n_features,n_timestep,output_values)\n",
    "\n",
    "#x_batch = torch.tensor(model_X,dtype=torch.float32)    \n",
    "#y_batch = torch.tensor(target_Y,dtype=torch.float32)\n",
    "\n",
    "#mv_net.init_hidden(x_batch.size(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 529,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# output_model=mv_net(x_batch)\n",
    "# output_model=output_model.unsqueeze(1)\n",
    "# output_model.view(-1).shape\n",
    "# y_batch.shape\n",
    "# #.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 530,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "input.size(-1) must be equal to input_size. Expected 21, got 2",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-530-5300167a574a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;31m#    lstm_out, _ = mv_net.l_lstm(x_batch,nnet.hidden)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;31m#    lstm_out.contiguous().view(x_batch.size(0),-1)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmv_net\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-528-e03eec6ab37e>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseq_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m         \u001b[0mlstm_out\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ml_lstm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhidden\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m         \u001b[0;31m# lstm_out(with batch_first = True) is\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0;31m# (batch_size,seq_len,num_directions * hidden_size)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    565\u001b[0m             \u001b[0mhx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute_hidden\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msorted_indices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    566\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 567\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_forward_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_sizes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    568\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbatch_sizes\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    569\u001b[0m             result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mcheck_forward_args\u001b[0;34m(self, input, hidden, batch_sizes)\u001b[0m\n\u001b[1;32m    517\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcheck_forward_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_sizes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    518\u001b[0m         \u001b[0;31m# type: (Tensor, Tuple[Tensor, Tensor], Optional[Tensor]) -> None\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 519\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_sizes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    520\u001b[0m         \u001b[0mexpected_hidden_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_expected_hidden_size\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_sizes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    521\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mcheck_input\u001b[0;34m(self, input, batch_sizes)\u001b[0m\n\u001b[1;32m    168\u001b[0m             raise RuntimeError(\n\u001b[1;32m    169\u001b[0m                 'input.size(-1) must be equal to input_size. Expected {}, got {}'.format(\n\u001b[0;32m--> 170\u001b[0;31m                     self.input_size, input.size(-1)))\n\u001b[0m\u001b[1;32m    171\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_expected_hidden_size\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_sizes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: input.size(-1) must be equal to input_size. Expected 21, got 2"
     ]
    }
   ],
   "source": [
    "mv_net.train()\n",
    "for t in range(train_episodes):\n",
    "    for b in range(0,len(X),batch_size):\n",
    "        optimizer.zero_grad()\n",
    "        inpt = X[b:b+batch_size,:,:]\n",
    "        target = y[b:b+batch_size]    \n",
    "        \n",
    "        x_batch = torch.tensor(inpt,dtype=torch.float32)    \n",
    "        y_batch = torch.tensor(target,dtype=torch.float32)\n",
    "    \n",
    "        mv_net.init_hidden(x_batch.size(0))\n",
    "    #    lstm_out, _ = mv_net.l_lstm(x_batch,nnet.hidden)    \n",
    "    #    lstm_out.contiguous().view(x_batch.size(0),-1)\n",
    "        output = mv_net(x_batch) \n",
    "        loss = criterion(output.view(-1), y_batch)  \n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()        \n",
    "        optimizer.zero_grad()\n",
    "    if t%50==0:\n",
    "        print('step : ' , t , 'loss : ' , loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8, 1, 3)"
      ]
     },
     "execution_count": 285,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 1, 3])"
      ]
     },
     "execution_count": 290,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_model.unsqueeze(1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 263,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7, 4, 2)"
      ]
     },
     "execution_count": 258,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 249,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_timesteps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MV_LSTM(\n",
       "  (l_lstm): LSTM(6, 20, batch_first=True)\n",
       "  (l_linear): Linear(in_features=80, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 243,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mv_net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 244,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_timestep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 2, 6])"
      ]
     },
     "execution_count": 240,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_batch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 40])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([[-1.4605e-10,  3.6516e-15, -3.9177e-08, -1.2141e-03,  1.9423e-13,\n",
       "          -1.8112e-03,  4.5679e-13, -2.5195e-07,  4.3536e-01,  7.6159e-01,\n",
       "           1.0594e-05, -7.1467e-05, -8.0673e-11,  2.0541e-07,  1.4053e-05,\n",
       "          -2.2703e-05, -1.6628e-02, -7.6018e-01,  6.2527e-15, -5.1137e-22,\n",
       "          -2.9734e-06,  2.2910e-08, -4.6745e-07, -1.9301e-01,  9.7930e-04,\n",
       "          -7.4684e-03,  7.6220e-03, -8.8545e-05,  6.2699e-04,  8.4632e-01,\n",
       "           2.9151e-04, -2.8093e-02,  2.5670e-10,  1.0551e-03,  1.9921e-04,\n",
       "          -6.8159e-01, -3.0499e-03, -5.2496e-01,  5.9434e-06, -9.6904e-11],\n",
       "         [-4.0565e-06,  2.6649e-08, -3.2313e-07, -1.7887e-01,  1.0403e-03,\n",
       "          -5.3878e-03,  8.2386e-03, -1.2562e-04,  3.9073e-04,  7.5997e-01,\n",
       "           2.0435e-04, -3.4006e-02,  3.8039e-10,  8.3854e-04,  2.1725e-04,\n",
       "          -6.8613e-01,  3.4383e-03, -4.2503e-01,  6.8033e-06, -1.1435e-10,\n",
       "           6.5860e-01,  2.5244e-08, -8.4974e-13, -8.6308e-01,  9.5599e-03,\n",
       "           4.3556e-01,  1.3902e-04, -3.7446e-05,  5.7464e-09,  9.4902e-01,\n",
       "           4.9223e-06, -1.2873e-03,  8.3564e-10,  6.6948e-08,  8.4433e-01,\n",
       "          -7.6090e-01,  7.2320e-01, -7.2993e-01, -4.2016e-04, -9.7837e-10],\n",
       "         [ 6.7864e-01, -7.0146e-10, -6.3105e-13, -2.4177e-01,  1.0005e-02,\n",
       "           4.2476e-01,  1.3532e-04, -4.7842e-05,  4.2377e-09,  7.6154e-01,\n",
       "           3.9377e-06, -1.2771e-12,  1.0937e-09,  5.2418e-11,  6.7306e-01,\n",
       "          -7.6096e-01,  7.0792e-01, -1.7643e-04, -6.1986e-04, -9.4813e-10,\n",
       "           1.1479e-12, -7.2194e-10, -8.4668e-05, -1.0150e-04,  1.4237e-11,\n",
       "           3.8256e-01,  7.5330e-11, -8.4035e-02,  1.5490e-08,  8.0169e-01,\n",
       "           6.1941e-11, -5.8282e-03,  2.5916e-15,  4.4538e-09,  8.6886e-13,\n",
       "          -2.9172e-01,  1.2009e-02, -4.8312e-01,  1.4512e-20, -7.2454e-24],\n",
       "         [-4.6110e-21,  8.5343e-25, -1.4234e-18, -1.2633e-04,  1.8027e-14,\n",
       "          -2.3791e-07,  1.6758e-11, -3.6679e-13,  1.3590e-08,  7.6159e-01,\n",
       "           6.7721e-11, -6.9474e-03,  5.1641e-27,  5.4422e-09,  8.2372e-13,\n",
       "          -3.4543e-01, -3.3169e-09, -4.9111e-01,  1.9995e-20, -8.8138e-35,\n",
       "          -1.9530e-22,  2.4470e-13, -8.7376e-10, -2.5648e-15,  3.8345e-20,\n",
       "          -1.3612e-08,  1.4985e-21, -3.9197e-17,  9.6330e-01,  2.8129e-07,\n",
       "           4.0759e-02,  7.5391e-01, -8.3984e-16,  4.5960e-01,  8.0335e-14,\n",
       "          -2.0947e-11, -3.4673e-10, -2.4507e-04,  4.3733e-22, -4.7534e-29],\n",
       "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00, -0.0000e+00,  0.0000e+00,\n",
       "           0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  7.6159e-01,\n",
       "           0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "          -7.6159e-01,  0.0000e+00, -7.6159e-01,  0.0000e+00,  0.0000e+00,\n",
       "           0.0000e+00,  0.0000e+00,  0.0000e+00, -0.0000e+00,  0.0000e+00,\n",
       "           0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  7.6159e-01,\n",
       "           0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "          -7.6159e-01,  0.0000e+00, -9.6403e-01,  0.0000e+00,  0.0000e+00],\n",
       "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00, -0.0000e+00,  0.0000e+00,\n",
       "           0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  7.6159e-01,\n",
       "           0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "          -7.6159e-01,  0.0000e+00, -7.6159e-01,  0.0000e+00,  0.0000e+00,\n",
       "           0.0000e+00,  0.0000e+00,  0.0000e+00, -0.0000e+00,  0.0000e+00,\n",
       "           0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  7.6159e-01,\n",
       "           0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "          -7.6159e-01,  0.0000e+00, -9.6403e-01,  0.0000e+00,  0.0000e+00],\n",
       "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00, -0.0000e+00,  0.0000e+00,\n",
       "           0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  7.6159e-01,\n",
       "           0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "          -7.6159e-01,  0.0000e+00, -7.6159e-01,  0.0000e+00,  0.0000e+00,\n",
       "           0.0000e+00,  0.0000e+00,  0.0000e+00, -0.0000e+00,  0.0000e+00,\n",
       "           0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  7.6159e-01,\n",
       "           0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "          -7.6159e-01,  0.0000e+00, -9.6403e-01,  0.0000e+00,  0.0000e+00],\n",
       "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00, -0.0000e+00,  0.0000e+00,\n",
       "           0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  7.6159e-01,\n",
       "           0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "          -7.6159e-01,  0.0000e+00, -7.6159e-01,  0.0000e+00,  0.0000e+00,\n",
       "           0.0000e+00,  0.0000e+00,  0.0000e+00, -0.0000e+00,  0.0000e+00,\n",
       "           0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  7.6159e-01,\n",
       "           0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "          -7.6159e-01,  0.0000e+00, -5.4020e-01,  0.0000e+00,  0.0000e+00]],\n",
       "        grad_fn=<ViewBackward>), tensor([[ 0.0240],\n",
       "         [ 0.2086],\n",
       "         [ 0.1333],\n",
       "         [-0.0335],\n",
       "         [ 0.1301],\n",
       "         [ 0.1301],\n",
       "         [ 0.1301],\n",
       "         [ 0.0794]], grad_fn=<AddmmBackward>))"
      ]
     },
     "execution_count": 273,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[6.481000e+01, 0.000000e+00, 0.000000e+00, 1.019400e+02,\n",
       "         0.000000e+00, 4.499000e+01],\n",
       "        [4.941000e+01, 0.000000e+00, 0.000000e+00, 5.222000e+01,\n",
       "         0.000000e+00, 0.000000e+00]],\n",
       "\n",
       "       [[4.941000e+01, 0.000000e+00, 0.000000e+00, 5.222000e+01,\n",
       "         0.000000e+00, 0.000000e+00],\n",
       "        [7.702000e+01, 0.000000e+00, 0.000000e+00, 0.000000e+00,\n",
       "         0.000000e+00, 3.303000e+01]],\n",
       "\n",
       "       [[7.702000e+01, 0.000000e+00, 0.000000e+00, 0.000000e+00,\n",
       "         0.000000e+00, 3.303000e+01],\n",
       "        [1.365600e+02, 0.000000e+00, 0.000000e+00, 1.864300e+02,\n",
       "         0.000000e+00, 9.990000e+00]],\n",
       "\n",
       "       [[1.365600e+02, 0.000000e+00, 0.000000e+00, 1.864300e+02,\n",
       "         0.000000e+00, 9.990000e+00],\n",
       "        [8.380000e+00, 0.000000e+00, 0.000000e+00, 2.086700e+02,\n",
       "         0.000000e+00, 0.000000e+00]],\n",
       "\n",
       "       [[1.373642e+04, 8.368000e+01, 5.900000e+01, 1.515324e+04,\n",
       "         1.586490e+03, 5.368500e+02],\n",
       "        [1.406572e+04, 7.835000e+01, 1.259200e+02, 1.629361e+04,\n",
       "         9.697000e+02, 9.095100e+02]],\n",
       "\n",
       "       [[1.406572e+04, 7.835000e+01, 1.259200e+02, 1.629361e+04,\n",
       "         9.697000e+02, 9.095100e+02],\n",
       "        [2.128144e+04, 1.036400e+02, 9.458900e+02, 2.185648e+04,\n",
       "         1.334330e+03, 1.703180e+03]],\n",
       "\n",
       "       [[2.128144e+04, 1.036400e+02, 9.458900e+02, 2.185648e+04,\n",
       "         1.334330e+03, 1.703180e+03],\n",
       "        [1.349032e+04, 6.460000e+01, 3.807200e+02, 1.435087e+04,\n",
       "         1.044800e+03, 8.367200e+02]],\n",
       "\n",
       "       [[1.349032e+04, 6.460000e+01, 3.807200e+02, 1.435087e+04,\n",
       "         1.044800e+03, 8.367200e+02],\n",
       "        [1.098936e+04, 3.885000e+01, 2.249300e+02, 1.323701e+04,\n",
       "         6.788500e+02, 3.948200e+02]]])"
      ]
     },
     "execution_count": 225,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'int' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-227-f53e772bd6d7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmv_net\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_X\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-220-84b2a2662136>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m         \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseq_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0mlstm_out\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ml_lstm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhidden\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'int' object is not callable"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 554,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3000, 3, 2)"
      ]
     },
     "execution_count": 554,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_array.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature='Direct_OPS'\n",
    "\n",
    "i=time_steps[0]  #target i \n",
    "feature_end=i-1\n",
    "feature_start=feature_end-seq_frame\n",
    "feature_window=list(np.arange(feature_start,feature_end))\n",
    " \n",
    "y=df_sample_1.loc[ :,(feature,slice(None),month_values[i])]\n",
    "#X=df_sample_1.loc[ :,(feature,slice(None),[array(month_values)[[1,2]]])]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"3\" halign=\"left\">Direct_OPS</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gl_top_30</th>\n",
       "      <th>147.0</th>\n",
       "      <th>-999</th>\n",
       "      <th>201.0</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>first_date</th>\n",
       "      <th>2019-11-01</th>\n",
       "      <th>2019-11-01</th>\n",
       "      <th>2019-11-01</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>store_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>072304-20</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>07301994-20</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>090288020101-20</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10150309-20</th>\n",
       "      <td>0.00</td>\n",
       "      <td>1580.34</td>\n",
       "      <td>81.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101mediagroup-20</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zikuzgun08-20</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zionlion02-20</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zionpillars-20</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zlifehealth00-20</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zlinefitness-20</th>\n",
       "      <td>17.01</td>\n",
       "      <td>375.00</td>\n",
       "      <td>78.98</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3000 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Direct_OPS                      \n",
       "gl_top_30             147.0       -999      201.0\n",
       "first_date       2019-11-01 2019-11-01 2019-11-01\n",
       "store_id                                         \n",
       "072304-20              0.00       0.00       0.00\n",
       "07301994-20            0.00       0.00       0.00\n",
       "090288020101-20        0.00       0.00       0.00\n",
       "10150309-20            0.00    1580.34      81.80\n",
       "101mediagroup-20       0.00       0.00       0.00\n",
       "...                     ...        ...        ...\n",
       "zikuzgun08-20          0.00       0.00       0.00\n",
       "zionlion02-20          0.00       0.00       0.00\n",
       "zionpillars-20         0.00       0.00       0.00\n",
       "zlifehealth00-20       0.00       0.00       0.00\n",
       "zlinefitness-20       17.01     375.00      78.98\n",
       "\n",
       "[3000 rows x 3 columns]"
      ]
     },
     "execution_count": 316,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sample_1.loc[ :,('Direct_OPS',slice(None),['2019-11-01'])]\n",
    "#df_sample_1.xs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean Code\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## reading in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch.utils.data import DataLoader ,TensorDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 515,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_halo=pd.read_csv('halo_vs_non_halo_ops_ccp.csv')\n",
    "\n",
    "df_sample=df_halo.merge(df_halo[['store_id']].drop_duplicates().sample(30000),\n",
    "                       on='store_id',\n",
    "                       how='inner')\n",
    "del df_halo\n",
    "gl_top_30=[147,201,121,200,79,21,193,194,60,14,107,23,421,263,86,468,469,199,309,229,196,325,328,267,75,63,351,504,198,228]\n",
    "df_sample['gl_top_30']=np.where(df_sample['gl'].isin(gl_top_30[:6]),\n",
    "                                df_sample['gl'],\n",
    "                                '-999'\n",
    "                               )\n",
    "columns=list(set(df_sample['gl_top_30'].values))\n",
    "\n",
    "df_sample[['Direct_OPS','Halo_OPS','total_ops','ops','net_ccp','gross_cplf']]=df_sample[['Direct_OPS','Halo_OPS','total_ops','ops','net_ccp','gross_cplf']].fillna(0)\n",
    "df_sample['first_date']=pd.to_datetime(df_sample['first_date'])\n",
    "df_sample_1=df_sample.groupby(['store_id','first_date','gl_top_30']).agg({'Direct_OPS':sum,\n",
    "                                                             'Halo_OPS':sum,\n",
    "                                                             'total_ops':sum,\n",
    "                                                             'net_ccp':sum}).reset_index()\n",
    "df_sample_1=df_sample_1.set_index(['store_id','first_date','gl_top_30']).unstack(level=[2,1]).fillna(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 516,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names=['Direct_OPS','Halo_OPS','total_ops']\n",
    "#target_feature=['Direct_OPS']\n",
    "target_feature=['total_ops']\n",
    "df_sample_2=df_sample_1.swaplevel(0,2,axis=1).swaplevel(1,2,axis=1).stack(0)\n",
    "feature=df_sample_2.swaplevel(1,0,axis=0)[feature_names]\n",
    "feature.columns=feature.columns.map('_'.join).str.strip('|')\n",
    "feature_names=feature.columns\n",
    "target_names=df_sample_2.swaplevel(1,0,axis=0)[target_feature].columns.map('_'.join).str.strip('|')\n",
    "feature_df=feature.reset_index()\n",
    "store_feature_1=feature_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 518,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_to_index='first_date'\n",
    "month_seris=pd.Series(sorted(set(feature_df[column_to_index])))\n",
    "len_time_steps=3\n",
    "starting_numeric_index=0\n",
    "ending_numeric_index=len(month_seris)-len_time_steps\n",
    "non_features=[i for i in feature_df.columns if i not in feature_names]\n",
    "non_features_for_modelling=[]\n",
    "features_for_modelling=[]\n",
    "target_for_modelling=[]\n",
    "for store in set(store_feature_1.store_id):\n",
    "    store_feature=store_feature_1[store_feature_1['store_id']==store]\n",
    "    for i in np.arange(starting_numeric_index,ending_numeric_index):\n",
    "        #print(i)\n",
    "        #print(month_seris[i:i+len_time_steps])\n",
    "        store_feature_for_the_iteration=store_feature[store_feature[column_to_index].isin(month_seris[i:i+len_time_steps])][feature_names]\n",
    "        non_store_feature_for_the_iteration=store_feature[store_feature[column_to_index].isin(month_seris[i:i+len_time_steps])][non_features]\n",
    "        target_feature_for_the_iteration=store_feature[store_feature[column_to_index]==month_seris[i+len_time_steps]][target_names]\n",
    "        features_for_modelling.append(store_feature_for_the_iteration.values)\n",
    "        non_features_for_modelling.append(non_store_feature_for_the_iteration.values)\n",
    "        target_for_modelling.append(target_feature_for_the_iteration.values)\n",
    "\n",
    "model_X=np.array(features_for_modelling)\n",
    "target_Y=np.array(target_for_modelling)\n",
    "non_feature_array=np.array(non_features_for_modelling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 531,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jauharim/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:55: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "/Users/jauharim/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
     ]
    }
   ],
   "source": [
    "class MV_LSTM(torch.nn.Module):\n",
    "    def __init__(self,n_features,seq_length,output_values=1):\n",
    "        super(MV_LSTM, self).__init__()\n",
    "        self.n_features = n_features\n",
    "        self.seq_len = seq_length\n",
    "        self.n_hidden = 20 # number of hidden states\n",
    "        self.n_layers = 1 # number of LSTM layers (stacked)\n",
    "        self.output_values=output_values\n",
    "        \n",
    "        self.l_lstm = torch.nn.LSTM(input_size = n_features, \n",
    "                                 hidden_size = self.n_hidden,\n",
    "                                 num_layers = self.n_layers, \n",
    "                                 batch_first = True)\n",
    "        # according to pytorch docs LSTM output is \n",
    "        # (batch_size,seq_len, num_directions * hidden_size)\n",
    "        # when considering batch_first = True\n",
    "        #print(self.n_hidden)\n",
    "        #print(self.seq_len)\n",
    "        #print(seq_length)\n",
    "        #print(self.n_hidden*self.seq_len)\n",
    "        self.l_linear = torch.nn.Linear(self.n_hidden*self.seq_len, output_values)\n",
    "        \n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        # even with batch_first = True this remains same as docs\n",
    "        hidden_state = torch.zeros(self.n_layers,batch_size,self.n_hidden)\n",
    "        cell_state = torch.zeros(self.n_layers,batch_size,self.n_hidden)\n",
    "        self.hidden = (hidden_state, cell_state)\n",
    "    \n",
    "    \n",
    "    def forward(self, x):        \n",
    "        batch_size, seq_len, _ = x.size()\n",
    "        \n",
    "        lstm_out, self.hidden = self.l_lstm(x,self.hidden)\n",
    "        # lstm_out(with batch_first = True) is \n",
    "        # (batch_size,seq_len,num_directions * hidden_size)\n",
    "        # for following linear layer we want to keep batch_size dimension and merge rest       \n",
    "        # .contiguous() -> solves tensor compatibility error\n",
    "        x = lstm_out.contiguous().view(batch_size,-1)\n",
    "        #print(x.shape)\n",
    "        #out=self.l_linear(x)\n",
    "        x=F.relu(x)\n",
    "        return self.l_linear(x)\n",
    "        #return x,out\n",
    "    \n",
    "criterion = torch.nn.MSELoss() # reduction='sum' created huge loss value\n",
    "optimizer = torch.optim.Adam(mv_net.parameters(), lr=1e-1)\n",
    "   \n",
    "n_features=model_X.shape[2]\n",
    "n_timestep=model_X.shape[1]\n",
    "output_values=target_Y.shape[2]\n",
    "mv_net = MV_LSTM(n_features,n_timestep,output_values)\n",
    "\n",
    "\n",
    "model_X=torch.tensor(model_X,dtype=torch.float32)\n",
    "target_Y=torch.tensor(target_Y,dtype=torch.float32)\n",
    "index_tensor=torch.from_numpy(np.arange(model_X.shape[0]))\n",
    "dataset = torch.utils.data.TensorDataset(model_X, target_Y,index_tensor)\n",
    "dataloader = DataLoader(dataset, batch_size=128, shuffle=True)\n",
    "\n",
    "#non_feature_array=torch.tensor(non_feature_array)\n",
    "#feat_list=[non_feature_array,model_X,target_Y]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 488,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iterator=iter(dataloader)\n",
    "# x_iter, y_iter,i_iter=iterator.next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 521,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# mv_net.eval()\n",
    "# mv_net.init_hidden(x_iter.size(0))\n",
    "# output=mv_net(x_iter).unsqueeze(1)\n",
    "# loss = criterion(output, y_iter)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 532,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch :  0 batch_num :  50 loss :  8036.84\n",
      "epoch :  0 batch_num :  100 loss :  158354.12\n",
      "epoch :  0 batch_num :  150 loss :  6999.55\n",
      "epoch :  0 batch_num :  200 loss :  64271.29\n",
      "epoch :  0 batch_num :  250 loss :  20776.21\n",
      "epoch :  0 batch_num :  300 loss :  2984.34\n",
      "epoch :  0 batch_num :  350 loss :  21304.76\n",
      "epoch :  0 batch_num :  400 loss :  2806.18\n",
      "epoch :  0 batch_num :  450 loss :  13500.81\n",
      "epoch :  0 batch_num :  500 loss :  4298.8\n",
      "epoch :  0 batch_num :  550 loss :  187572.11\n",
      "epoch :  0 batch_num :  600 loss :  23665.32\n",
      "epoch :  0 batch_num :  650 loss :  5285.75\n",
      "epoch :  0 batch_num :  700 loss :  2302.96\n",
      "epoch :  20 batch_num :  50 loss :  33552.47\n",
      "epoch :  20 batch_num :  100 loss :  5289.3\n",
      "epoch :  20 batch_num :  150 loss :  155073.97\n",
      "epoch :  20 batch_num :  200 loss :  11525.14\n",
      "epoch :  20 batch_num :  250 loss :  1984.87\n",
      "epoch :  20 batch_num :  300 loss :  7138.45\n",
      "epoch :  20 batch_num :  350 loss :  19598.7\n",
      "epoch :  20 batch_num :  400 loss :  49992.24\n",
      "epoch :  20 batch_num :  450 loss :  10406.52\n",
      "epoch :  20 batch_num :  500 loss :  1223.1\n",
      "epoch :  20 batch_num :  550 loss :  44094.79\n",
      "epoch :  20 batch_num :  600 loss :  20585.33\n",
      "epoch :  20 batch_num :  650 loss :  257047.53\n",
      "epoch :  20 batch_num :  700 loss :  5982.79\n",
      "epoch :  40 batch_num :  50 loss :  1981.22\n",
      "epoch :  40 batch_num :  100 loss :  13694.65\n",
      "epoch :  40 batch_num :  150 loss :  4901.18\n",
      "epoch :  40 batch_num :  200 loss :  7416.58\n",
      "epoch :  40 batch_num :  250 loss :  42142.81\n",
      "epoch :  40 batch_num :  300 loss :  10227.99\n",
      "epoch :  40 batch_num :  350 loss :  17852.52\n",
      "epoch :  40 batch_num :  400 loss :  3425.48\n",
      "epoch :  40 batch_num :  450 loss :  1254.26\n",
      "epoch :  40 batch_num :  500 loss :  7143.09\n",
      "epoch :  40 batch_num :  550 loss :  1326.67\n",
      "epoch :  40 batch_num :  600 loss :  3371.21\n",
      "epoch :  40 batch_num :  650 loss :  19418.55\n",
      "epoch :  40 batch_num :  700 loss :  7002.57\n",
      "epoch :  60 batch_num :  50 loss :  9474.25\n",
      "epoch :  60 batch_num :  100 loss :  364464.19\n",
      "epoch :  60 batch_num :  150 loss :  26765.26\n",
      "epoch :  60 batch_num :  200 loss :  1163.42\n",
      "epoch :  60 batch_num :  250 loss :  1294.96\n",
      "epoch :  60 batch_num :  300 loss :  2473.84\n",
      "epoch :  60 batch_num :  350 loss :  3897.01\n",
      "epoch :  60 batch_num :  400 loss :  6546.8\n",
      "epoch :  60 batch_num :  450 loss :  5625.79\n",
      "epoch :  60 batch_num :  500 loss :  35604.28\n",
      "epoch :  60 batch_num :  550 loss :  9276.85\n",
      "epoch :  60 batch_num :  600 loss :  3393.74\n",
      "epoch :  60 batch_num :  650 loss :  20764.74\n",
      "epoch :  60 batch_num :  700 loss :  18483.71\n",
      "epoch :  80 batch_num :  50 loss :  289281.69\n",
      "epoch :  80 batch_num :  100 loss :  4645.29\n",
      "epoch :  80 batch_num :  150 loss :  17927.25\n",
      "epoch :  80 batch_num :  200 loss :  1997.53\n",
      "epoch :  80 batch_num :  250 loss :  12341.67\n",
      "epoch :  80 batch_num :  300 loss :  84110.01\n",
      "epoch :  80 batch_num :  350 loss :  2114.08\n",
      "epoch :  80 batch_num :  400 loss :  555.33\n",
      "epoch :  80 batch_num :  450 loss :  31018.85\n",
      "epoch :  80 batch_num :  500 loss :  810548.12\n",
      "epoch :  80 batch_num :  550 loss :  6201.66\n",
      "epoch :  80 batch_num :  600 loss :  6281.52\n",
      "epoch :  80 batch_num :  650 loss :  2911840.0\n",
      "epoch :  80 batch_num :  700 loss :  823.99\n",
      "epoch :  100 batch_num :  50 loss :  428.55\n",
      "epoch :  100 batch_num :  100 loss :  80933.91\n",
      "epoch :  100 batch_num :  150 loss :  1572154.25\n",
      "epoch :  100 batch_num :  200 loss :  2364.4\n",
      "epoch :  100 batch_num :  250 loss :  9494.87\n",
      "epoch :  100 batch_num :  300 loss :  1032.18\n",
      "epoch :  100 batch_num :  350 loss :  54167.29\n",
      "epoch :  100 batch_num :  400 loss :  750.64\n",
      "epoch :  100 batch_num :  450 loss :  1017.1\n",
      "epoch :  100 batch_num :  500 loss :  11377.38\n",
      "epoch :  100 batch_num :  550 loss :  1062.79\n",
      "epoch :  100 batch_num :  600 loss :  1304.64\n",
      "epoch :  100 batch_num :  650 loss :  5167.73\n",
      "epoch :  100 batch_num :  700 loss :  4093.49\n",
      "epoch :  120 batch_num :  50 loss :  395.59\n",
      "epoch :  120 batch_num :  100 loss :  8622.61\n",
      "epoch :  120 batch_num :  150 loss :  147253.53\n",
      "epoch :  120 batch_num :  200 loss :  16399.63\n",
      "epoch :  120 batch_num :  250 loss :  2134759.5\n",
      "epoch :  120 batch_num :  300 loss :  2580.52\n",
      "epoch :  120 batch_num :  350 loss :  393.04\n",
      "epoch :  120 batch_num :  400 loss :  45923.54\n",
      "epoch :  120 batch_num :  450 loss :  1415.67\n",
      "epoch :  120 batch_num :  500 loss :  78356.42\n",
      "epoch :  120 batch_num :  550 loss :  4657.32\n",
      "epoch :  120 batch_num :  600 loss :  1120.95\n",
      "epoch :  120 batch_num :  650 loss :  80341.73\n",
      "epoch :  120 batch_num :  700 loss :  3762.82\n",
      "epoch :  140 batch_num :  50 loss :  71532.88\n",
      "epoch :  140 batch_num :  100 loss :  6660.47\n",
      "epoch :  140 batch_num :  150 loss :  57403.91\n",
      "epoch :  140 batch_num :  200 loss :  25087.57\n",
      "epoch :  140 batch_num :  250 loss :  51266.6\n",
      "epoch :  140 batch_num :  300 loss :  4673.17\n",
      "epoch :  140 batch_num :  350 loss :  1005.63\n",
      "epoch :  140 batch_num :  400 loss :  805.37\n",
      "epoch :  140 batch_num :  450 loss :  132694.27\n",
      "epoch :  140 batch_num :  500 loss :  40885.47\n",
      "epoch :  140 batch_num :  550 loss :  5479.78\n",
      "epoch :  140 batch_num :  600 loss :  329655.47\n",
      "epoch :  140 batch_num :  650 loss :  148799.48\n",
      "epoch :  140 batch_num :  700 loss :  14741.93\n",
      "epoch :  160 batch_num :  50 loss :  16284.83\n",
      "epoch :  160 batch_num :  100 loss :  13280.6\n",
      "epoch :  160 batch_num :  150 loss :  2694.44\n",
      "epoch :  160 batch_num :  200 loss :  37487.15\n",
      "epoch :  160 batch_num :  250 loss :  20702.25\n",
      "epoch :  160 batch_num :  300 loss :  9877.71\n",
      "epoch :  160 batch_num :  350 loss :  20800.17\n",
      "epoch :  160 batch_num :  400 loss :  22046.89\n",
      "epoch :  160 batch_num :  450 loss :  18067.91\n",
      "epoch :  160 batch_num :  500 loss :  33818.7\n",
      "epoch :  160 batch_num :  550 loss :  15757.56\n",
      "epoch :  160 batch_num :  600 loss :  2767153.75\n",
      "epoch :  160 batch_num :  650 loss :  7595.48\n",
      "epoch :  160 batch_num :  700 loss :  1722.45\n",
      "epoch :  180 batch_num :  50 loss :  1378747.75\n",
      "epoch :  180 batch_num :  100 loss :  185623.67\n",
      "epoch :  180 batch_num :  150 loss :  4825067.5\n",
      "epoch :  180 batch_num :  200 loss :  156067.75\n",
      "epoch :  180 batch_num :  250 loss :  23564.75\n",
      "epoch :  180 batch_num :  300 loss :  22885.9\n",
      "epoch :  180 batch_num :  350 loss :  118369.54\n",
      "epoch :  180 batch_num :  400 loss :  8399.07\n",
      "epoch :  180 batch_num :  450 loss :  21851.29\n",
      "epoch :  180 batch_num :  500 loss :  25671.76\n",
      "epoch :  180 batch_num :  550 loss :  6864.25\n",
      "epoch :  180 batch_num :  600 loss :  35830.59\n",
      "epoch :  180 batch_num :  650 loss :  11279.38\n",
      "epoch :  180 batch_num :  700 loss :  1358.76\n",
      "epoch :  200 batch_num :  50 loss :  81703.42\n",
      "epoch :  200 batch_num :  100 loss :  12686.46\n",
      "epoch :  200 batch_num :  150 loss :  48596.89\n",
      "epoch :  200 batch_num :  200 loss :  1070.59\n",
      "epoch :  200 batch_num :  250 loss :  304.32\n",
      "epoch :  200 batch_num :  300 loss :  4959.97\n",
      "epoch :  200 batch_num :  350 loss :  16458.75\n",
      "epoch :  200 batch_num :  400 loss :  44793.46\n",
      "epoch :  200 batch_num :  450 loss :  74972.36\n",
      "epoch :  200 batch_num :  500 loss :  10284.74\n",
      "epoch :  200 batch_num :  550 loss :  8990.91\n",
      "epoch :  200 batch_num :  600 loss :  19553.89\n",
      "epoch :  200 batch_num :  650 loss :  2493.09\n",
      "epoch :  200 batch_num :  700 loss :  29796.03\n",
      "epoch :  220 batch_num :  50 loss :  41808.8\n",
      "epoch :  220 batch_num :  100 loss :  52489.48\n",
      "epoch :  220 batch_num :  150 loss :  5343.84\n",
      "epoch :  220 batch_num :  200 loss :  48932.85\n",
      "epoch :  220 batch_num :  250 loss :  160455.5\n",
      "epoch :  220 batch_num :  300 loss :  1788.2\n",
      "epoch :  220 batch_num :  350 loss :  64149.76\n",
      "epoch :  220 batch_num :  400 loss :  136.43\n",
      "epoch :  220 batch_num :  450 loss :  16692.24\n",
      "epoch :  220 batch_num :  500 loss :  53511.55\n",
      "epoch :  220 batch_num :  550 loss :  417592.34\n",
      "epoch :  220 batch_num :  600 loss :  7968.1\n",
      "epoch :  220 batch_num :  650 loss :  73658.15\n",
      "epoch :  220 batch_num :  700 loss :  4302.79\n",
      "epoch :  240 batch_num :  50 loss :  1514.87\n",
      "epoch :  240 batch_num :  100 loss :  1589.6\n",
      "epoch :  240 batch_num :  150 loss :  3849.42\n",
      "epoch :  240 batch_num :  200 loss :  9332.17\n",
      "epoch :  240 batch_num :  250 loss :  20735.16\n",
      "epoch :  240 batch_num :  300 loss :  17650.16\n",
      "epoch :  240 batch_num :  350 loss :  14107.29\n",
      "epoch :  240 batch_num :  400 loss :  1445.8\n",
      "epoch :  240 batch_num :  450 loss :  12636.22\n",
      "epoch :  240 batch_num :  500 loss :  9137.48\n",
      "epoch :  240 batch_num :  550 loss :  3708.77\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch :  240 batch_num :  600 loss :  982.7\n",
      "epoch :  240 batch_num :  650 loss :  8548.77\n",
      "epoch :  240 batch_num :  700 loss :  38918.98\n",
      "epoch :  260 batch_num :  50 loss :  32774.84\n",
      "epoch :  260 batch_num :  100 loss :  92353.17\n",
      "epoch :  260 batch_num :  150 loss :  6733.41\n",
      "epoch :  260 batch_num :  200 loss :  51968.75\n",
      "epoch :  260 batch_num :  250 loss :  48152.19\n",
      "epoch :  260 batch_num :  300 loss :  129620.5\n",
      "epoch :  260 batch_num :  350 loss :  12698.43\n",
      "epoch :  260 batch_num :  400 loss :  2833.74\n",
      "epoch :  260 batch_num :  450 loss :  92978.75\n",
      "epoch :  260 batch_num :  500 loss :  2202.34\n",
      "epoch :  260 batch_num :  550 loss :  110.93\n",
      "epoch :  260 batch_num :  600 loss :  3931.75\n",
      "epoch :  260 batch_num :  650 loss :  6969.25\n",
      "epoch :  260 batch_num :  700 loss :  2027.11\n",
      "epoch :  280 batch_num :  50 loss :  2643.87\n",
      "epoch :  280 batch_num :  100 loss :  53911.79\n",
      "epoch :  280 batch_num :  150 loss :  14320.77\n",
      "epoch :  280 batch_num :  200 loss :  491715.5\n",
      "epoch :  280 batch_num :  250 loss :  7726.06\n",
      "epoch :  280 batch_num :  300 loss :  20523.13\n",
      "epoch :  280 batch_num :  350 loss :  1845.85\n",
      "epoch :  280 batch_num :  400 loss :  4294.49\n",
      "epoch :  280 batch_num :  450 loss :  36243.65\n",
      "epoch :  280 batch_num :  500 loss :  12077.75\n",
      "epoch :  280 batch_num :  550 loss :  129021.26\n",
      "epoch :  280 batch_num :  600 loss :  30409.41\n",
      "epoch :  280 batch_num :  650 loss :  6566.69\n",
      "epoch :  280 batch_num :  700 loss :  12384.02\n",
      "epoch :  300 batch_num :  50 loss :  42031.5\n",
      "epoch :  300 batch_num :  100 loss :  31837.4\n",
      "epoch :  300 batch_num :  150 loss :  59510.72\n",
      "epoch :  300 batch_num :  200 loss :  12064.17\n",
      "epoch :  300 batch_num :  250 loss :  6532.52\n",
      "epoch :  300 batch_num :  300 loss :  10815.9\n",
      "epoch :  300 batch_num :  350 loss :  18507.27\n",
      "epoch :  300 batch_num :  400 loss :  1592.28\n",
      "epoch :  300 batch_num :  450 loss :  33937.14\n",
      "epoch :  300 batch_num :  500 loss :  4265.85\n",
      "epoch :  300 batch_num :  550 loss :  53109.43\n",
      "epoch :  300 batch_num :  600 loss :  26144.99\n",
      "epoch :  300 batch_num :  650 loss :  1438.28\n",
      "epoch :  300 batch_num :  700 loss :  1449.72\n",
      "epoch :  320 batch_num :  50 loss :  33103.93\n",
      "epoch :  320 batch_num :  100 loss :  14632.64\n",
      "epoch :  320 batch_num :  150 loss :  600.51\n",
      "epoch :  320 batch_num :  200 loss :  26603.4\n",
      "epoch :  320 batch_num :  250 loss :  2297.35\n",
      "epoch :  320 batch_num :  300 loss :  1041.09\n",
      "epoch :  320 batch_num :  350 loss :  948732.69\n",
      "epoch :  320 batch_num :  400 loss :  6372.23\n",
      "epoch :  320 batch_num :  450 loss :  2092.92\n",
      "epoch :  320 batch_num :  500 loss :  67661.45\n",
      "epoch :  320 batch_num :  550 loss :  366207.78\n",
      "epoch :  320 batch_num :  600 loss :  908410.81\n",
      "epoch :  320 batch_num :  650 loss :  9946.5\n",
      "epoch :  320 batch_num :  700 loss :  3314.75\n",
      "epoch :  340 batch_num :  50 loss :  7551.95\n",
      "epoch :  340 batch_num :  100 loss :  9984.71\n",
      "epoch :  340 batch_num :  150 loss :  30270.88\n",
      "epoch :  340 batch_num :  200 loss :  152563.95\n",
      "epoch :  340 batch_num :  250 loss :  10055.69\n",
      "epoch :  340 batch_num :  300 loss :  884266.88\n",
      "epoch :  340 batch_num :  350 loss :  3088.58\n",
      "epoch :  340 batch_num :  400 loss :  2677.81\n",
      "epoch :  340 batch_num :  450 loss :  9034.05\n",
      "epoch :  340 batch_num :  500 loss :  2138.94\n",
      "epoch :  340 batch_num :  550 loss :  3323.74\n",
      "epoch :  340 batch_num :  600 loss :  551705.12\n",
      "epoch :  340 batch_num :  650 loss :  12485.87\n",
      "epoch :  340 batch_num :  700 loss :  1561720.12\n",
      "epoch :  360 batch_num :  50 loss :  913.15\n",
      "epoch :  360 batch_num :  100 loss :  20932.05\n",
      "epoch :  360 batch_num :  150 loss :  3449.21\n",
      "epoch :  360 batch_num :  200 loss :  27423.92\n",
      "epoch :  360 batch_num :  250 loss :  8687.71\n",
      "epoch :  360 batch_num :  300 loss :  2897.35\n",
      "epoch :  360 batch_num :  350 loss :  32488.22\n",
      "epoch :  360 batch_num :  400 loss :  16018.67\n",
      "epoch :  360 batch_num :  450 loss :  6495.74\n",
      "epoch :  360 batch_num :  500 loss :  604430.44\n",
      "epoch :  360 batch_num :  550 loss :  161129.83\n",
      "epoch :  360 batch_num :  600 loss :  5543.13\n",
      "epoch :  360 batch_num :  650 loss :  208.1\n",
      "epoch :  360 batch_num :  700 loss :  24924.59\n",
      "epoch :  380 batch_num :  50 loss :  2499203.25\n",
      "epoch :  380 batch_num :  100 loss :  5838.79\n",
      "epoch :  380 batch_num :  150 loss :  17625.55\n",
      "epoch :  380 batch_num :  200 loss :  1706.0\n",
      "epoch :  380 batch_num :  250 loss :  1513.66\n",
      "epoch :  380 batch_num :  300 loss :  1153.29\n",
      "epoch :  380 batch_num :  350 loss :  390.7\n",
      "epoch :  380 batch_num :  400 loss :  68654.73\n",
      "epoch :  380 batch_num :  450 loss :  5656.96\n",
      "epoch :  380 batch_num :  500 loss :  6556.48\n",
      "epoch :  380 batch_num :  550 loss :  9168.88\n",
      "epoch :  380 batch_num :  600 loss :  32470.52\n",
      "epoch :  380 batch_num :  650 loss :  21594.97\n",
      "epoch :  380 batch_num :  700 loss :  2124.14\n",
      "epoch :  400 batch_num :  50 loss :  36487.32\n",
      "epoch :  400 batch_num :  100 loss :  334509.34\n",
      "epoch :  400 batch_num :  150 loss :  544014.88\n",
      "epoch :  400 batch_num :  200 loss :  70161.8\n",
      "epoch :  400 batch_num :  250 loss :  12847.08\n",
      "epoch :  400 batch_num :  300 loss :  10576.94\n",
      "epoch :  400 batch_num :  350 loss :  2390.02\n",
      "epoch :  400 batch_num :  400 loss :  6597.66\n",
      "epoch :  400 batch_num :  450 loss :  4336.27\n",
      "epoch :  400 batch_num :  500 loss :  2956.62\n",
      "epoch :  400 batch_num :  550 loss :  23308.31\n",
      "epoch :  400 batch_num :  600 loss :  5536.56\n",
      "epoch :  400 batch_num :  650 loss :  17342502.0\n",
      "epoch :  400 batch_num :  700 loss :  423500.97\n",
      "epoch :  420 batch_num :  50 loss :  23785.62\n",
      "epoch :  420 batch_num :  100 loss :  4417.64\n",
      "epoch :  420 batch_num :  150 loss :  30716.57\n",
      "epoch :  420 batch_num :  200 loss :  4520.2\n",
      "epoch :  420 batch_num :  250 loss :  1794.97\n",
      "epoch :  420 batch_num :  300 loss :  7024.6\n",
      "epoch :  420 batch_num :  350 loss :  1820.76\n",
      "epoch :  420 batch_num :  400 loss :  1359220.38\n",
      "epoch :  420 batch_num :  450 loss :  4370.21\n",
      "epoch :  420 batch_num :  500 loss :  2423.75\n",
      "epoch :  420 batch_num :  550 loss :  1450.6\n",
      "epoch :  420 batch_num :  600 loss :  172752.67\n",
      "epoch :  420 batch_num :  650 loss :  1292.57\n",
      "epoch :  420 batch_num :  700 loss :  82684.84\n",
      "epoch :  440 batch_num :  50 loss :  87578.24\n",
      "epoch :  440 batch_num :  100 loss :  22154.92\n",
      "epoch :  440 batch_num :  150 loss :  1061183.75\n",
      "epoch :  440 batch_num :  200 loss :  46895.97\n",
      "epoch :  440 batch_num :  250 loss :  7502.5\n",
      "epoch :  440 batch_num :  300 loss :  255.9\n",
      "epoch :  440 batch_num :  350 loss :  22181.72\n",
      "epoch :  440 batch_num :  400 loss :  5985.83\n",
      "epoch :  440 batch_num :  450 loss :  11283.56\n",
      "epoch :  440 batch_num :  500 loss :  235552.11\n",
      "epoch :  440 batch_num :  550 loss :  31601.58\n",
      "epoch :  440 batch_num :  600 loss :  21885.98\n",
      "epoch :  440 batch_num :  650 loss :  9866.83\n",
      "epoch :  440 batch_num :  700 loss :  2123.76\n",
      "epoch :  460 batch_num :  50 loss :  240710.14\n",
      "epoch :  460 batch_num :  100 loss :  13805.95\n",
      "epoch :  460 batch_num :  150 loss :  2922.95\n",
      "epoch :  460 batch_num :  200 loss :  2578.73\n",
      "epoch :  460 batch_num :  250 loss :  309703.28\n",
      "epoch :  460 batch_num :  300 loss :  3269.9\n",
      "epoch :  460 batch_num :  350 loss :  4936.12\n",
      "epoch :  460 batch_num :  400 loss :  105305.82\n",
      "epoch :  460 batch_num :  450 loss :  20285.25\n",
      "epoch :  460 batch_num :  500 loss :  3292.15\n",
      "epoch :  460 batch_num :  550 loss :  1187.11\n",
      "epoch :  460 batch_num :  600 loss :  43334.15\n",
      "epoch :  460 batch_num :  650 loss :  1243.4\n",
      "epoch :  460 batch_num :  700 loss :  3121.07\n",
      "epoch :  480 batch_num :  50 loss :  512.32\n",
      "epoch :  480 batch_num :  100 loss :  4444.44\n",
      "epoch :  480 batch_num :  150 loss :  2071.93\n",
      "epoch :  480 batch_num :  200 loss :  46647.91\n",
      "epoch :  480 batch_num :  250 loss :  19903.57\n",
      "epoch :  480 batch_num :  300 loss :  9948.89\n",
      "epoch :  480 batch_num :  350 loss :  286.71\n",
      "epoch :  480 batch_num :  400 loss :  693.17\n",
      "epoch :  480 batch_num :  450 loss :  1891.55\n",
      "epoch :  480 batch_num :  500 loss :  7400.36\n",
      "epoch :  480 batch_num :  550 loss :  7566.87\n",
      "epoch :  480 batch_num :  600 loss :  5488.81\n",
      "epoch :  480 batch_num :  650 loss :  2037.04\n",
      "epoch :  480 batch_num :  700 loss :  43780.74\n"
     ]
    }
   ],
   "source": [
    "loss_list=[]\n",
    "epochs=500\n",
    "mv_net.train()\n",
    "for e in range(epochs):\n",
    "    batch_num=0\n",
    "    for x_iter, y_iter,i_iter in dataloader:\n",
    "        batch_num=batch_num+1\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        mv_net.init_hidden(x_iter.size(0))\n",
    "        output=mv_net(x_iter).unsqueeze(1)\n",
    "        loss = criterion(output, y_iter)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step() \n",
    "        if batch_num%50==0:\n",
    "            loss_list.append(round(loss.item()/len(i_iter),2))\n",
    "        if e%20==0 and batch_num%50==0:\n",
    "            print('epoch : ',e, 'batch_num : ' , batch_num , 'loss : ' , round(loss.item()/len(i_iter),2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 533,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x150b59190>"
      ]
     },
     "execution_count": 533,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "pd.DataFrame({'loss':loss_list},index=np.arange(len(loss_list))).plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# with mean absolute error and clipping gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 535,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jauharim/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:55: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "/Users/jauharim/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
     ]
    }
   ],
   "source": [
    "class MV_LSTM(torch.nn.Module):\n",
    "    def __init__(self,n_features,seq_length,output_values=1):\n",
    "        super(MV_LSTM, self).__init__()\n",
    "        self.n_features = n_features\n",
    "        self.seq_len = seq_length\n",
    "        self.n_hidden = 20 # number of hidden states\n",
    "        self.n_layers = 1 # number of LSTM layers (stacked)\n",
    "        self.output_values=output_values\n",
    "    \n",
    "        self.l_lstm = torch.nn.LSTM(input_size = n_features, \n",
    "                                 hidden_size = self.n_hidden,\n",
    "                                 num_layers = self.n_layers, \n",
    "                                 batch_first = True)\n",
    "        # according to pytorch docs LSTM output is \n",
    "        # (batch_size,seq_len, num_directions * hidden_size)\n",
    "        # when considering batch_first = True\n",
    "        #print(self.n_hidden)\n",
    "        #print(self.seq_len)\n",
    "        #print(seq_length)\n",
    "        #print(self.n_hidden*self.seq_len)\n",
    "        self.l_linear = torch.nn.Linear(self.n_hidden*self.seq_len, output_values)\n",
    "        \n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        # even with batch_first = True this remains same as docs\n",
    "        hidden_state = torch.zeros(self.n_layers,batch_size,self.n_hidden)\n",
    "        cell_state = torch.zeros(self.n_layers,batch_size,self.n_hidden)\n",
    "        self.hidden = (hidden_state, cell_state)\n",
    "    \n",
    "    \n",
    "    def forward(self, x):        \n",
    "        batch_size, seq_len, _ = x.size()\n",
    "        \n",
    "        lstm_out, self.hidden = self.l_lstm(x,self.hidden)\n",
    "        # lstm_out(with batch_first = True) is \n",
    "        # (batch_size,seq_len,num_directions * hidden_size)\n",
    "        # for following linear layer we want to keep batch_size dimension and merge rest       \n",
    "        # .contiguous() -> solves tensor compatibility error\n",
    "        x = lstm_out.contiguous().view(batch_size,-1)\n",
    "        #print(x.shape)\n",
    "        #out=self.l_linear(x)\n",
    "        x=F.relu(x)\n",
    "        return self.l_linear(x)\n",
    "        #return x,out\n",
    "    \n",
    "criterion = torch.nn.L1Loss() # \n",
    "optimizer = torch.optim.Adam(mv_net.parameters(), lr=1e-1)\n",
    "   \n",
    "n_features=model_X.shape[2]\n",
    "n_timestep=model_X.shape[1]\n",
    "output_values=target_Y.shape[2]\n",
    "mv_net = MV_LSTM(n_features,n_timestep,output_values)\n",
    "\n",
    "\n",
    "model_X=torch.tensor(model_X,dtype=torch.float32)\n",
    "target_Y=torch.tensor(target_Y,dtype=torch.float32)\n",
    "index_tensor=torch.from_numpy(np.arange(model_X.shape[0]))\n",
    "dataset = torch.utils.data.TensorDataset(model_X, target_Y,index_tensor)\n",
    "dataloader = DataLoader(dataset, batch_size=128, shuffle=True)\n",
    "\n",
    "#non_feature_array=torch.tensor(non_feature_array)\n",
    "#feat_list=[non_feature_array,model_X,target_Y]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 536,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jauharim/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:16: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "  app.launch_new_instance()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch :  0 batch_num :  50 loss :  0.66\n",
      "epoch :  0 batch_num :  100 loss :  2.0\n",
      "epoch :  0 batch_num :  150 loss :  0.5\n",
      "epoch :  0 batch_num :  200 loss :  0.71\n",
      "epoch :  0 batch_num :  250 loss :  0.59\n",
      "epoch :  0 batch_num :  300 loss :  1.05\n",
      "epoch :  0 batch_num :  350 loss :  1.79\n",
      "epoch :  0 batch_num :  400 loss :  0.97\n",
      "epoch :  0 batch_num :  450 loss :  1.2\n",
      "epoch :  0 batch_num :  500 loss :  1.14\n",
      "epoch :  0 batch_num :  550 loss :  0.7\n",
      "epoch :  0 batch_num :  600 loss :  0.25\n",
      "epoch :  0 batch_num :  650 loss :  1.47\n",
      "epoch :  0 batch_num :  700 loss :  0.33\n",
      "epoch :  20 batch_num :  50 loss :  1.54\n",
      "epoch :  20 batch_num :  100 loss :  0.97\n",
      "epoch :  20 batch_num :  150 loss :  0.73\n",
      "epoch :  20 batch_num :  200 loss :  0.39\n",
      "epoch :  20 batch_num :  250 loss :  1.49\n",
      "epoch :  20 batch_num :  300 loss :  1.32\n",
      "epoch :  20 batch_num :  350 loss :  1.67\n",
      "epoch :  20 batch_num :  400 loss :  0.79\n",
      "epoch :  20 batch_num :  450 loss :  0.81\n",
      "epoch :  20 batch_num :  500 loss :  2.9\n",
      "epoch :  20 batch_num :  550 loss :  0.89\n",
      "epoch :  20 batch_num :  600 loss :  1.13\n",
      "epoch :  20 batch_num :  650 loss :  1.63\n",
      "epoch :  20 batch_num :  700 loss :  0.5\n",
      "epoch :  40 batch_num :  50 loss :  1.38\n",
      "epoch :  40 batch_num :  100 loss :  0.87\n",
      "epoch :  40 batch_num :  150 loss :  0.68\n",
      "epoch :  40 batch_num :  200 loss :  1.26\n",
      "epoch :  40 batch_num :  250 loss :  1.91\n",
      "epoch :  40 batch_num :  300 loss :  1.65\n",
      "epoch :  40 batch_num :  350 loss :  0.42\n",
      "epoch :  40 batch_num :  400 loss :  3.34\n",
      "epoch :  40 batch_num :  450 loss :  3.36\n",
      "epoch :  40 batch_num :  500 loss :  0.68\n",
      "epoch :  40 batch_num :  550 loss :  0.44\n",
      "epoch :  40 batch_num :  600 loss :  13.53\n",
      "epoch :  40 batch_num :  650 loss :  0.4\n",
      "epoch :  40 batch_num :  700 loss :  12.83\n",
      "epoch :  60 batch_num :  50 loss :  0.36\n",
      "epoch :  60 batch_num :  100 loss :  1.09\n",
      "epoch :  60 batch_num :  150 loss :  1.73\n",
      "epoch :  60 batch_num :  200 loss :  1.23\n",
      "epoch :  60 batch_num :  250 loss :  0.47\n",
      "epoch :  60 batch_num :  300 loss :  2.07\n",
      "epoch :  60 batch_num :  350 loss :  0.3\n",
      "epoch :  60 batch_num :  400 loss :  0.85\n",
      "epoch :  60 batch_num :  450 loss :  2.09\n",
      "epoch :  60 batch_num :  500 loss :  1.28\n",
      "epoch :  60 batch_num :  550 loss :  0.91\n",
      "epoch :  60 batch_num :  600 loss :  0.5\n",
      "epoch :  60 batch_num :  650 loss :  1.11\n",
      "epoch :  60 batch_num :  700 loss :  0.27\n",
      "epoch :  80 batch_num :  50 loss :  0.9\n",
      "epoch :  80 batch_num :  100 loss :  0.84\n",
      "epoch :  80 batch_num :  150 loss :  3.13\n",
      "epoch :  80 batch_num :  200 loss :  0.89\n",
      "epoch :  80 batch_num :  250 loss :  0.75\n",
      "epoch :  80 batch_num :  300 loss :  2.76\n",
      "epoch :  80 batch_num :  350 loss :  0.68\n",
      "epoch :  80 batch_num :  400 loss :  0.85\n",
      "epoch :  80 batch_num :  450 loss :  0.66\n",
      "epoch :  80 batch_num :  500 loss :  5.62\n",
      "epoch :  80 batch_num :  550 loss :  0.77\n",
      "epoch :  80 batch_num :  600 loss :  0.87\n",
      "epoch :  80 batch_num :  650 loss :  1.48\n",
      "epoch :  80 batch_num :  700 loss :  2.74\n",
      "epoch :  100 batch_num :  50 loss :  0.44\n",
      "epoch :  100 batch_num :  100 loss :  1.01\n",
      "epoch :  100 batch_num :  150 loss :  1.1\n",
      "epoch :  100 batch_num :  200 loss :  0.49\n",
      "epoch :  100 batch_num :  250 loss :  3.47\n",
      "epoch :  100 batch_num :  300 loss :  0.88\n",
      "epoch :  100 batch_num :  350 loss :  1.23\n",
      "epoch :  100 batch_num :  400 loss :  1.03\n",
      "epoch :  100 batch_num :  450 loss :  0.63\n",
      "epoch :  100 batch_num :  500 loss :  1.08\n",
      "epoch :  100 batch_num :  550 loss :  0.55\n",
      "epoch :  100 batch_num :  600 loss :  1.13\n",
      "epoch :  100 batch_num :  650 loss :  0.49\n",
      "epoch :  100 batch_num :  700 loss :  1.75\n",
      "epoch :  120 batch_num :  50 loss :  1.2\n",
      "epoch :  120 batch_num :  100 loss :  0.68\n",
      "epoch :  120 batch_num :  150 loss :  2.52\n",
      "epoch :  120 batch_num :  200 loss :  0.84\n",
      "epoch :  120 batch_num :  250 loss :  0.66\n",
      "epoch :  120 batch_num :  300 loss :  11.68\n",
      "epoch :  120 batch_num :  350 loss :  2.58\n",
      "epoch :  120 batch_num :  400 loss :  4.72\n",
      "epoch :  120 batch_num :  450 loss :  0.71\n",
      "epoch :  120 batch_num :  500 loss :  1.4\n",
      "epoch :  120 batch_num :  550 loss :  0.65\n",
      "epoch :  120 batch_num :  600 loss :  1.9\n",
      "epoch :  120 batch_num :  650 loss :  0.79\n",
      "epoch :  120 batch_num :  700 loss :  0.64\n",
      "epoch :  140 batch_num :  50 loss :  0.91\n",
      "epoch :  140 batch_num :  100 loss :  1.69\n",
      "epoch :  140 batch_num :  150 loss :  0.56\n",
      "epoch :  140 batch_num :  200 loss :  0.46\n",
      "epoch :  140 batch_num :  250 loss :  0.47\n",
      "epoch :  140 batch_num :  300 loss :  0.9\n",
      "epoch :  140 batch_num :  350 loss :  0.24\n",
      "epoch :  140 batch_num :  400 loss :  2.15\n",
      "epoch :  140 batch_num :  450 loss :  0.74\n",
      "epoch :  140 batch_num :  500 loss :  1.38\n",
      "epoch :  140 batch_num :  550 loss :  0.43\n",
      "epoch :  140 batch_num :  600 loss :  0.95\n",
      "epoch :  140 batch_num :  650 loss :  0.43\n",
      "epoch :  140 batch_num :  700 loss :  2.15\n",
      "epoch :  160 batch_num :  50 loss :  2.52\n",
      "epoch :  160 batch_num :  100 loss :  0.67\n",
      "epoch :  160 batch_num :  150 loss :  0.62\n",
      "epoch :  160 batch_num :  200 loss :  0.18\n",
      "epoch :  160 batch_num :  250 loss :  0.57\n",
      "epoch :  160 batch_num :  300 loss :  0.59\n",
      "epoch :  160 batch_num :  350 loss :  0.7\n",
      "epoch :  160 batch_num :  400 loss :  0.49\n",
      "epoch :  160 batch_num :  450 loss :  1.53\n",
      "epoch :  160 batch_num :  500 loss :  0.83\n",
      "epoch :  160 batch_num :  550 loss :  0.79\n",
      "epoch :  160 batch_num :  600 loss :  0.24\n",
      "epoch :  160 batch_num :  650 loss :  0.78\n",
      "epoch :  160 batch_num :  700 loss :  0.46\n",
      "epoch :  180 batch_num :  50 loss :  1.22\n",
      "epoch :  180 batch_num :  100 loss :  1.3\n",
      "epoch :  180 batch_num :  150 loss :  3.94\n",
      "epoch :  180 batch_num :  200 loss :  3.72\n",
      "epoch :  180 batch_num :  250 loss :  0.81\n",
      "epoch :  180 batch_num :  300 loss :  0.87\n",
      "epoch :  180 batch_num :  350 loss :  0.44\n",
      "epoch :  180 batch_num :  400 loss :  0.26\n",
      "epoch :  180 batch_num :  450 loss :  0.28\n",
      "epoch :  180 batch_num :  500 loss :  0.63\n",
      "epoch :  180 batch_num :  550 loss :  0.51\n",
      "epoch :  180 batch_num :  600 loss :  2.11\n",
      "epoch :  180 batch_num :  650 loss :  14.13\n",
      "epoch :  180 batch_num :  700 loss :  0.62\n",
      "epoch :  200 batch_num :  50 loss :  3.37\n",
      "epoch :  200 batch_num :  100 loss :  1.81\n",
      "epoch :  200 batch_num :  150 loss :  0.64\n",
      "epoch :  200 batch_num :  200 loss :  0.9\n",
      "epoch :  200 batch_num :  250 loss :  3.52\n",
      "epoch :  200 batch_num :  300 loss :  0.48\n",
      "epoch :  200 batch_num :  350 loss :  1.08\n",
      "epoch :  200 batch_num :  400 loss :  0.99\n",
      "epoch :  200 batch_num :  450 loss :  1.43\n",
      "epoch :  200 batch_num :  500 loss :  16.06\n",
      "epoch :  200 batch_num :  550 loss :  1.67\n",
      "epoch :  200 batch_num :  600 loss :  1.15\n",
      "epoch :  200 batch_num :  650 loss :  0.68\n",
      "epoch :  200 batch_num :  700 loss :  0.37\n",
      "epoch :  220 batch_num :  50 loss :  1.4\n",
      "epoch :  220 batch_num :  100 loss :  1.58\n",
      "epoch :  220 batch_num :  150 loss :  1.7\n",
      "epoch :  220 batch_num :  200 loss :  0.28\n",
      "epoch :  220 batch_num :  250 loss :  0.55\n",
      "epoch :  220 batch_num :  300 loss :  0.48\n",
      "epoch :  220 batch_num :  350 loss :  1.66\n",
      "epoch :  220 batch_num :  400 loss :  1.22\n",
      "epoch :  220 batch_num :  450 loss :  1.24\n",
      "epoch :  220 batch_num :  500 loss :  3.09\n",
      "epoch :  220 batch_num :  550 loss :  1.12\n",
      "epoch :  220 batch_num :  600 loss :  0.79\n",
      "epoch :  220 batch_num :  650 loss :  6.44\n",
      "epoch :  220 batch_num :  700 loss :  1.21\n",
      "epoch :  240 batch_num :  50 loss :  0.84\n",
      "epoch :  240 batch_num :  100 loss :  0.78\n",
      "epoch :  240 batch_num :  150 loss :  8.25\n",
      "epoch :  240 batch_num :  200 loss :  2.66\n",
      "epoch :  240 batch_num :  250 loss :  0.58\n",
      "epoch :  240 batch_num :  300 loss :  0.41\n",
      "epoch :  240 batch_num :  350 loss :  1.03\n",
      "epoch :  240 batch_num :  400 loss :  1.11\n",
      "epoch :  240 batch_num :  450 loss :  1.22\n",
      "epoch :  240 batch_num :  500 loss :  0.58\n",
      "epoch :  240 batch_num :  550 loss :  2.02\n",
      "epoch :  240 batch_num :  600 loss :  1.61\n",
      "epoch :  240 batch_num :  650 loss :  2.0\n",
      "epoch :  240 batch_num :  700 loss :  0.54\n",
      "epoch :  260 batch_num :  50 loss :  1.17\n",
      "epoch :  260 batch_num :  100 loss :  3.62\n",
      "epoch :  260 batch_num :  150 loss :  1.63\n",
      "epoch :  260 batch_num :  200 loss :  1.78\n",
      "epoch :  260 batch_num :  250 loss :  0.39\n",
      "epoch :  260 batch_num :  300 loss :  0.96\n",
      "epoch :  260 batch_num :  350 loss :  0.46\n",
      "epoch :  260 batch_num :  400 loss :  5.12\n",
      "epoch :  260 batch_num :  450 loss :  4.4\n",
      "epoch :  260 batch_num :  500 loss :  2.15\n",
      "epoch :  260 batch_num :  550 loss :  0.5\n",
      "epoch :  260 batch_num :  600 loss :  1.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch :  260 batch_num :  650 loss :  5.74\n",
      "epoch :  260 batch_num :  700 loss :  1.54\n",
      "epoch :  280 batch_num :  50 loss :  13.12\n",
      "epoch :  280 batch_num :  100 loss :  2.5\n",
      "epoch :  280 batch_num :  150 loss :  0.85\n",
      "epoch :  280 batch_num :  200 loss :  2.74\n",
      "epoch :  280 batch_num :  250 loss :  6.1\n",
      "epoch :  280 batch_num :  300 loss :  0.81\n",
      "epoch :  280 batch_num :  350 loss :  1.17\n",
      "epoch :  280 batch_num :  400 loss :  0.63\n",
      "epoch :  280 batch_num :  450 loss :  0.82\n",
      "epoch :  280 batch_num :  500 loss :  0.83\n",
      "epoch :  280 batch_num :  550 loss :  0.96\n",
      "epoch :  280 batch_num :  600 loss :  5.05\n",
      "epoch :  280 batch_num :  650 loss :  0.25\n",
      "epoch :  280 batch_num :  700 loss :  2.59\n",
      "epoch :  300 batch_num :  50 loss :  1.33\n",
      "epoch :  300 batch_num :  100 loss :  1.2\n",
      "epoch :  300 batch_num :  150 loss :  1.35\n",
      "epoch :  300 batch_num :  200 loss :  0.74\n",
      "epoch :  300 batch_num :  250 loss :  8.06\n",
      "epoch :  300 batch_num :  300 loss :  0.5\n",
      "epoch :  300 batch_num :  350 loss :  0.38\n",
      "epoch :  300 batch_num :  400 loss :  0.6\n",
      "epoch :  300 batch_num :  450 loss :  7.1\n",
      "epoch :  300 batch_num :  500 loss :  1.94\n",
      "epoch :  300 batch_num :  550 loss :  1.29\n",
      "epoch :  300 batch_num :  600 loss :  0.95\n",
      "epoch :  300 batch_num :  650 loss :  0.97\n",
      "epoch :  300 batch_num :  700 loss :  1.53\n",
      "epoch :  320 batch_num :  50 loss :  0.75\n",
      "epoch :  320 batch_num :  100 loss :  0.95\n",
      "epoch :  320 batch_num :  150 loss :  0.96\n",
      "epoch :  320 batch_num :  200 loss :  1.5\n",
      "epoch :  320 batch_num :  250 loss :  1.17\n",
      "epoch :  320 batch_num :  300 loss :  3.81\n",
      "epoch :  320 batch_num :  350 loss :  1.25\n",
      "epoch :  320 batch_num :  400 loss :  3.22\n",
      "epoch :  320 batch_num :  450 loss :  1.82\n",
      "epoch :  320 batch_num :  500 loss :  0.56\n",
      "epoch :  320 batch_num :  550 loss :  0.88\n",
      "epoch :  320 batch_num :  600 loss :  1.81\n",
      "epoch :  320 batch_num :  650 loss :  0.4\n",
      "epoch :  320 batch_num :  700 loss :  1.63\n",
      "epoch :  340 batch_num :  50 loss :  1.63\n",
      "epoch :  340 batch_num :  100 loss :  0.94\n",
      "epoch :  340 batch_num :  150 loss :  1.62\n",
      "epoch :  340 batch_num :  200 loss :  1.28\n",
      "epoch :  340 batch_num :  250 loss :  0.78\n",
      "epoch :  340 batch_num :  300 loss :  0.83\n",
      "epoch :  340 batch_num :  350 loss :  0.69\n",
      "epoch :  340 batch_num :  400 loss :  0.88\n",
      "epoch :  340 batch_num :  450 loss :  0.53\n",
      "epoch :  340 batch_num :  500 loss :  0.59\n",
      "epoch :  340 batch_num :  550 loss :  1.46\n",
      "epoch :  340 batch_num :  600 loss :  0.63\n",
      "epoch :  340 batch_num :  650 loss :  0.34\n",
      "epoch :  340 batch_num :  700 loss :  0.74\n",
      "epoch :  360 batch_num :  50 loss :  1.54\n",
      "epoch :  360 batch_num :  100 loss :  0.95\n",
      "epoch :  360 batch_num :  150 loss :  0.84\n",
      "epoch :  360 batch_num :  200 loss :  0.78\n",
      "epoch :  360 batch_num :  250 loss :  0.74\n",
      "epoch :  360 batch_num :  300 loss :  0.34\n",
      "epoch :  360 batch_num :  350 loss :  1.09\n",
      "epoch :  360 batch_num :  400 loss :  0.43\n",
      "epoch :  360 batch_num :  450 loss :  0.7\n",
      "epoch :  360 batch_num :  500 loss :  0.42\n",
      "epoch :  360 batch_num :  550 loss :  3.57\n",
      "epoch :  360 batch_num :  600 loss :  0.45\n",
      "epoch :  360 batch_num :  650 loss :  0.81\n",
      "epoch :  360 batch_num :  700 loss :  0.9\n",
      "epoch :  380 batch_num :  50 loss :  1.06\n",
      "epoch :  380 batch_num :  100 loss :  0.86\n",
      "epoch :  380 batch_num :  150 loss :  0.53\n",
      "epoch :  380 batch_num :  200 loss :  1.03\n",
      "epoch :  380 batch_num :  250 loss :  0.55\n",
      "epoch :  380 batch_num :  300 loss :  6.37\n",
      "epoch :  380 batch_num :  350 loss :  0.72\n",
      "epoch :  380 batch_num :  400 loss :  0.5\n",
      "epoch :  380 batch_num :  450 loss :  1.75\n",
      "epoch :  380 batch_num :  500 loss :  0.72\n",
      "epoch :  380 batch_num :  550 loss :  0.6\n",
      "epoch :  380 batch_num :  600 loss :  6.0\n",
      "epoch :  380 batch_num :  650 loss :  0.91\n",
      "epoch :  380 batch_num :  700 loss :  6.57\n",
      "epoch :  400 batch_num :  50 loss :  3.71\n",
      "epoch :  400 batch_num :  100 loss :  0.53\n",
      "epoch :  400 batch_num :  150 loss :  0.28\n",
      "epoch :  400 batch_num :  200 loss :  0.19\n",
      "epoch :  400 batch_num :  250 loss :  6.0\n",
      "epoch :  400 batch_num :  300 loss :  0.3\n",
      "epoch :  400 batch_num :  350 loss :  0.72\n",
      "epoch :  400 batch_num :  400 loss :  1.28\n",
      "epoch :  400 batch_num :  450 loss :  0.79\n",
      "epoch :  400 batch_num :  500 loss :  0.52\n",
      "epoch :  400 batch_num :  550 loss :  1.91\n",
      "epoch :  400 batch_num :  600 loss :  0.25\n",
      "epoch :  400 batch_num :  650 loss :  1.89\n",
      "epoch :  400 batch_num :  700 loss :  1.25\n",
      "epoch :  420 batch_num :  50 loss :  9.35\n",
      "epoch :  420 batch_num :  100 loss :  1.07\n",
      "epoch :  420 batch_num :  150 loss :  1.81\n",
      "epoch :  420 batch_num :  200 loss :  0.44\n",
      "epoch :  420 batch_num :  250 loss :  0.28\n",
      "epoch :  420 batch_num :  300 loss :  0.45\n",
      "epoch :  420 batch_num :  350 loss :  0.36\n",
      "epoch :  420 batch_num :  400 loss :  0.7\n",
      "epoch :  420 batch_num :  450 loss :  0.53\n",
      "epoch :  420 batch_num :  500 loss :  0.79\n",
      "epoch :  420 batch_num :  550 loss :  0.26\n",
      "epoch :  420 batch_num :  600 loss :  1.77\n",
      "epoch :  420 batch_num :  650 loss :  0.49\n",
      "epoch :  420 batch_num :  700 loss :  0.44\n",
      "epoch :  440 batch_num :  50 loss :  0.97\n",
      "epoch :  440 batch_num :  100 loss :  0.23\n",
      "epoch :  440 batch_num :  150 loss :  1.9\n",
      "epoch :  440 batch_num :  200 loss :  2.79\n",
      "epoch :  440 batch_num :  250 loss :  1.08\n",
      "epoch :  440 batch_num :  300 loss :  0.89\n",
      "epoch :  440 batch_num :  350 loss :  2.02\n",
      "epoch :  440 batch_num :  400 loss :  1.15\n",
      "epoch :  440 batch_num :  450 loss :  0.6\n",
      "epoch :  440 batch_num :  500 loss :  0.71\n",
      "epoch :  440 batch_num :  550 loss :  0.43\n",
      "epoch :  440 batch_num :  600 loss :  0.63\n",
      "epoch :  440 batch_num :  650 loss :  0.78\n",
      "epoch :  440 batch_num :  700 loss :  0.41\n",
      "epoch :  460 batch_num :  50 loss :  6.4\n",
      "epoch :  460 batch_num :  100 loss :  0.38\n",
      "epoch :  460 batch_num :  150 loss :  0.66\n",
      "epoch :  460 batch_num :  200 loss :  0.93\n",
      "epoch :  460 batch_num :  250 loss :  1.03\n",
      "epoch :  460 batch_num :  300 loss :  0.99\n",
      "epoch :  460 batch_num :  350 loss :  0.7\n",
      "epoch :  460 batch_num :  400 loss :  1.14\n",
      "epoch :  460 batch_num :  450 loss :  0.88\n",
      "epoch :  460 batch_num :  500 loss :  0.39\n",
      "epoch :  460 batch_num :  550 loss :  0.58\n",
      "epoch :  460 batch_num :  600 loss :  4.2\n",
      "epoch :  460 batch_num :  650 loss :  0.48\n",
      "epoch :  460 batch_num :  700 loss :  0.4\n",
      "epoch :  480 batch_num :  50 loss :  0.52\n",
      "epoch :  480 batch_num :  100 loss :  1.26\n",
      "epoch :  480 batch_num :  150 loss :  2.17\n",
      "epoch :  480 batch_num :  200 loss :  0.65\n",
      "epoch :  480 batch_num :  250 loss :  0.52\n",
      "epoch :  480 batch_num :  300 loss :  1.49\n",
      "epoch :  480 batch_num :  350 loss :  0.46\n",
      "epoch :  480 batch_num :  400 loss :  4.07\n",
      "epoch :  480 batch_num :  450 loss :  2.78\n",
      "epoch :  480 batch_num :  500 loss :  1.12\n",
      "epoch :  480 batch_num :  550 loss :  1.0\n",
      "epoch :  480 batch_num :  600 loss :  0.28\n",
      "epoch :  480 batch_num :  650 loss :  0.96\n",
      "epoch :  480 batch_num :  700 loss :  1.4\n"
     ]
    }
   ],
   "source": [
    "clip_value=25\n",
    "loss_list=[]\n",
    "epochs=500\n",
    "mv_net.train()\n",
    "for e in range(epochs):\n",
    "    batch_num=0\n",
    "    for x_iter, y_iter,i_iter in dataloader:\n",
    "        batch_num=batch_num+1\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        mv_net.init_hidden(x_iter.size(0))\n",
    "        output=mv_net(x_iter).unsqueeze(1)\n",
    "        loss = criterion(output, y_iter)\n",
    "        \n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm(mv_net.parameters(), clip_value)\n",
    "\n",
    "        optimizer.step() \n",
    "        if batch_num%50==0:\n",
    "            loss_list.append(round(loss.item()/len(i_iter),2))\n",
    "        if e%20==0 and batch_num%50==0:\n",
    "            print('epoch : ',e, 'batch_num : ' , batch_num , 'loss : ' , round(loss.item()/len(i_iter),2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 538,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.66,\n",
       " 2.0,\n",
       " 0.5,\n",
       " 0.71,\n",
       " 0.59,\n",
       " 1.05,\n",
       " 1.79,\n",
       " 0.97,\n",
       " 1.2,\n",
       " 1.14,\n",
       " 0.7,\n",
       " 0.25,\n",
       " 1.47,\n",
       " 0.33,\n",
       " 1.95,\n",
       " 4.1,\n",
       " 2.55,\n",
       " 0.66,\n",
       " 1.4,\n",
       " 0.32,\n",
       " 0.92,\n",
       " 0.74,\n",
       " 0.94,\n",
       " 1.84,\n",
       " 1.45,\n",
       " 4.82,\n",
       " 1.16,\n",
       " 0.95,\n",
       " 0.91,\n",
       " 0.88,\n",
       " 3.22,\n",
       " 3.91,\n",
       " 1.38,\n",
       " 3.42,\n",
       " 0.92,\n",
       " 1.2,\n",
       " 6.2,\n",
       " 2.39,\n",
       " 1.07,\n",
       " 1.25,\n",
       " 0.94,\n",
       " 0.86,\n",
       " 5.23,\n",
       " 1.81,\n",
       " 0.46,\n",
       " 2.25,\n",
       " 0.86,\n",
       " 1.63,\n",
       " 2.93,\n",
       " 2.29,\n",
       " 0.38,\n",
       " 0.82,\n",
       " 0.98,\n",
       " 0.48,\n",
       " 0.5,\n",
       " 0.89,\n",
       " 0.73,\n",
       " 1.14,\n",
       " 0.87,\n",
       " 0.74,\n",
       " 0.55,\n",
       " 0.23,\n",
       " 1.93,\n",
       " 1.26,\n",
       " 0.69,\n",
       " 0.31,\n",
       " 0.78,\n",
       " 2.1,\n",
       " 0.98,\n",
       " 0.77,\n",
       " 0.99,\n",
       " 1.48,\n",
       " 0.92,\n",
       " 1.65,\n",
       " 4.08,\n",
       " 0.77,\n",
       " 0.72,\n",
       " 8.26,\n",
       " 11.3,\n",
       " 4.16,\n",
       " 1.64,\n",
       " 1.02,\n",
       " 0.65,\n",
       " 1.63,\n",
       " 1.14,\n",
       " 1.14,\n",
       " 0.98,\n",
       " 2.66,\n",
       " 0.36,\n",
       " 0.63,\n",
       " 0.3,\n",
       " 13.97,\n",
       " 0.48,\n",
       " 3.24,\n",
       " 0.37,\n",
       " 0.76,\n",
       " 0.37,\n",
       " 1.03,\n",
       " 1.72,\n",
       " 1.16,\n",
       " 5.92,\n",
       " 0.29,\n",
       " 0.99,\n",
       " 1.12,\n",
       " 0.35,\n",
       " 1.67,\n",
       " 0.66,\n",
       " 1.46,\n",
       " 13.58,\n",
       " 0.5,\n",
       " 1.14,\n",
       " 0.4,\n",
       " 0.85,\n",
       " 1.53,\n",
       " 0.35,\n",
       " 0.21,\n",
       " 0.34,\n",
       " 2.42,\n",
       " 1.37,\n",
       " 0.45,\n",
       " 1.34,\n",
       " 0.26,\n",
       " 1.17,\n",
       " 3.45,\n",
       " 0.97,\n",
       " 2.9,\n",
       " 1.69,\n",
       " 0.36,\n",
       " 0.21,\n",
       " 0.5,\n",
       " 4.08,\n",
       " 0.51,\n",
       " 2.11,\n",
       " 0.84,\n",
       " 15.56,\n",
       " 0.91,\n",
       " 2.6,\n",
       " 0.37,\n",
       " 0.77,\n",
       " 8.38,\n",
       " 1.35,\n",
       " 3.56,\n",
       " 0.7,\n",
       " 0.36,\n",
       " 0.53,\n",
       " 1.16,\n",
       " 1.86,\n",
       " 0.87,\n",
       " 1.04,\n",
       " 1.42,\n",
       " 4.2,\n",
       " 0.82,\n",
       " 0.67,\n",
       " 0.6,\n",
       " 0.94,\n",
       " 0.44,\n",
       " 0.92,\n",
       " 1.03,\n",
       " 0.87,\n",
       " 1.51,\n",
       " 0.52,\n",
       " 0.47,\n",
       " 0.77,\n",
       " 1.34,\n",
       " 1.1,\n",
       " 0.4,\n",
       " 6.64,\n",
       " 0.81,\n",
       " 0.54,\n",
       " 4.23,\n",
       " 0.64,\n",
       " 1.42,\n",
       " 2.35,\n",
       " 0.36,\n",
       " 0.92,\n",
       " 1.22,\n",
       " 0.6,\n",
       " 1.32,\n",
       " 6.63,\n",
       " 0.36,\n",
       " 0.46,\n",
       " 0.28,\n",
       " 2.42,\n",
       " 0.69,\n",
       " 0.87,\n",
       " 1.93,\n",
       " 0.49,\n",
       " 0.57,\n",
       " 5.84,\n",
       " 1.41,\n",
       " 0.91,\n",
       " 0.96,\n",
       " 1.96,\n",
       " 4.37,\n",
       " 0.57,\n",
       " 4.73,\n",
       " 0.34,\n",
       " 0.81,\n",
       " 9.41,\n",
       " 0.67,\n",
       " 0.36,\n",
       " 0.83,\n",
       " 0.99,\n",
       " 1.73,\n",
       " 1.47,\n",
       " 0.76,\n",
       " 1.74,\n",
       " 0.32,\n",
       " 2.88,\n",
       " 1.02,\n",
       " 15.48,\n",
       " 1.08,\n",
       " 1.98,\n",
       " 0.82,\n",
       " 1.64,\n",
       " 0.5,\n",
       " 1.44,\n",
       " 5.26,\n",
       " 0.39,\n",
       " 1.61,\n",
       " 1.04,\n",
       " 1.27,\n",
       " 3.42,\n",
       " 0.75,\n",
       " 1.91,\n",
       " 0.72,\n",
       " 1.65,\n",
       " 1.69,\n",
       " 0.45,\n",
       " 0.89,\n",
       " 0.71,\n",
       " 3.55,\n",
       " 0.75,\n",
       " 3.15,\n",
       " 0.51,\n",
       " 0.75,\n",
       " 1.39,\n",
       " 2.2,\n",
       " 2.28,\n",
       " 4.29,\n",
       " 1.56,\n",
       " 1.26,\n",
       " 2.82,\n",
       " 1.26,\n",
       " 1.24,\n",
       " 1.71,\n",
       " 0.43,\n",
       " 2.63,\n",
       " 1.71,\n",
       " 1.26,\n",
       " 0.85,\n",
       " 0.53,\n",
       " 0.45,\n",
       " 2.21,\n",
       " 3.02,\n",
       " 5.98,\n",
       " 0.57,\n",
       " 1.16,\n",
       " 0.2,\n",
       " 0.92,\n",
       " 0.8,\n",
       " 5.95,\n",
       " 0.44,\n",
       " 1.54,\n",
       " 0.42,\n",
       " 4.16,\n",
       " 1.48,\n",
       " 0.8,\n",
       " 0.39,\n",
       " 0.49,\n",
       " 0.61,\n",
       " 9.1,\n",
       " 0.4,\n",
       " 0.41,\n",
       " 0.49,\n",
       " 5.81,\n",
       " 8.2,\n",
       " 0.72,\n",
       " 0.61,\n",
       " 0.58,\n",
       " 1.54,\n",
       " 0.97,\n",
       " 0.73,\n",
       " 0.39,\n",
       " 1.49,\n",
       " 1.32,\n",
       " 1.67,\n",
       " 0.79,\n",
       " 0.81,\n",
       " 2.9,\n",
       " 0.89,\n",
       " 1.13,\n",
       " 1.63,\n",
       " 0.5,\n",
       " 3.39,\n",
       " 1.1,\n",
       " 0.4,\n",
       " 0.4,\n",
       " 0.73,\n",
       " 0.17,\n",
       " 0.78,\n",
       " 0.97,\n",
       " 1.67,\n",
       " 1.14,\n",
       " 1.3,\n",
       " 1.31,\n",
       " 0.26,\n",
       " 1.38,\n",
       " 1.96,\n",
       " 2.3,\n",
       " 1.48,\n",
       " 0.66,\n",
       " 0.61,\n",
       " 1.51,\n",
       " 1.42,\n",
       " 0.44,\n",
       " 2.01,\n",
       " 1.61,\n",
       " 1.19,\n",
       " 0.65,\n",
       " 0.76,\n",
       " 0.7,\n",
       " 0.52,\n",
       " 9.6,\n",
       " 1.02,\n",
       " 0.42,\n",
       " 0.6,\n",
       " 4.71,\n",
       " 1.16,\n",
       " 0.56,\n",
       " 0.25,\n",
       " 1.96,\n",
       " 0.65,\n",
       " 3.41,\n",
       " 1.07,\n",
       " 1.74,\n",
       " 1.37,\n",
       " 1.41,\n",
       " 0.32,\n",
       " 0.33,\n",
       " 0.45,\n",
       " 0.88,\n",
       " 0.7,\n",
       " 0.47,\n",
       " 0.59,\n",
       " 0.46,\n",
       " 2.57,\n",
       " 0.71,\n",
       " 0.82,\n",
       " 0.78,\n",
       " 0.77,\n",
       " 0.57,\n",
       " 0.69,\n",
       " 3.08,\n",
       " 0.65,\n",
       " 7.27,\n",
       " 1.18,\n",
       " 0.61,\n",
       " 1.55,\n",
       " 0.71,\n",
       " 0.84,\n",
       " 0.98,\n",
       " 1.6,\n",
       " 1.36,\n",
       " 0.83,\n",
       " 0.69,\n",
       " 0.69,\n",
       " 1.15,\n",
       " 1.83,\n",
       " 0.3,\n",
       " 1.7,\n",
       " 1.38,\n",
       " 0.65,\n",
       " 0.86,\n",
       " 0.59,\n",
       " 1.4,\n",
       " 1.12,\n",
       " 0.92,\n",
       " 0.55,\n",
       " 0.79,\n",
       " 0.78,\n",
       " 1.97,\n",
       " 1.92,\n",
       " 0.37,\n",
       " 0.57,\n",
       " 0.58,\n",
       " 2.66,\n",
       " 0.78,\n",
       " 0.46,\n",
       " 1.48,\n",
       " 1.4,\n",
       " 0.4,\n",
       " 0.7,\n",
       " 0.91,\n",
       " 0.58,\n",
       " 1.65,\n",
       " 0.39,\n",
       " 1.13,\n",
       " 3.06,\n",
       " 3.53,\n",
       " 0.96,\n",
       " 1.16,\n",
       " 3.51,\n",
       " 0.52,\n",
       " 0.9,\n",
       " 4.73,\n",
       " 0.69,\n",
       " 1.54,\n",
       " 9.18,\n",
       " 1.17,\n",
       " 2.64,\n",
       " 0.46,\n",
       " 1.45,\n",
       " 0.36,\n",
       " 1.79,\n",
       " 2.9,\n",
       " 0.35,\n",
       " 0.93,\n",
       " 0.42,\n",
       " 1.04,\n",
       " 0.61,\n",
       " 0.46,\n",
       " 1.32,\n",
       " 0.98,\n",
       " 0.39,\n",
       " 2.41,\n",
       " 4.39,\n",
       " 2.62,\n",
       " 0.79,\n",
       " 0.57,\n",
       " 1.75,\n",
       " 0.51,\n",
       " 1.7,\n",
       " 0.85,\n",
       " 0.85,\n",
       " 1.4,\n",
       " 1.7,\n",
       " 1.8,\n",
       " 1.88,\n",
       " 2.17,\n",
       " 0.44,\n",
       " 0.91,\n",
       " 0.49,\n",
       " 0.82,\n",
       " 1.09,\n",
       " 0.77,\n",
       " 2.78,\n",
       " 0.72,\n",
       " 1.35,\n",
       " 0.62,\n",
       " 0.34,\n",
       " 0.38,\n",
       " 1.08,\n",
       " 0.93,\n",
       " 5.91,\n",
       " 0.67,\n",
       " 0.99,\n",
       " 0.74,\n",
       " 0.76,\n",
       " 0.58,\n",
       " 0.81,\n",
       " 2.27,\n",
       " 0.67,\n",
       " 0.36,\n",
       " 0.58,\n",
       " 0.82,\n",
       " 2.38,\n",
       " 2.09,\n",
       " 0.8,\n",
       " 0.36,\n",
       " 1.12,\n",
       " 2.8,\n",
       " 1.51,\n",
       " 1.54,\n",
       " 0.68,\n",
       " 13.49,\n",
       " 1.27,\n",
       " 0.59,\n",
       " 0.6,\n",
       " 0.81,\n",
       " 0.86,\n",
       " 1.23,\n",
       " 1.57,\n",
       " 0.59,\n",
       " 0.59,\n",
       " 0.22,\n",
       " 0.32,\n",
       " 0.69,\n",
       " 1.61,\n",
       " 0.63,\n",
       " 0.98,\n",
       " 2.1,\n",
       " 0.3,\n",
       " 4.27,\n",
       " 6.42,\n",
       " 0.35,\n",
       " 2.76,\n",
       " 0.39,\n",
       " 1.21,\n",
       " 0.43,\n",
       " 1.6,\n",
       " 2.08,\n",
       " 0.39,\n",
       " 0.63,\n",
       " 1.52,\n",
       " 2.71,\n",
       " 13.13,\n",
       " 1.21,\n",
       " 1.07,\n",
       " 1.38,\n",
       " 1.14,\n",
       " 0.82,\n",
       " 1.62,\n",
       " 0.45,\n",
       " 0.57,\n",
       " 3.79,\n",
       " 0.45,\n",
       " 3.72,\n",
       " 1.18,\n",
       " 0.59,\n",
       " 0.33,\n",
       " 0.84,\n",
       " 1.28,\n",
       " 0.83,\n",
       " 0.78,\n",
       " 0.88,\n",
       " 1.37,\n",
       " 1.02,\n",
       " 0.36,\n",
       " 0.4,\n",
       " 1.28,\n",
       " 9.42,\n",
       " 4.36,\n",
       " 0.64,\n",
       " 0.39,\n",
       " 1.77,\n",
       " 2.0,\n",
       " 1.04,\n",
       " 1.36,\n",
       " 0.29,\n",
       " 2.41,\n",
       " 1.74,\n",
       " 0.95,\n",
       " 1.15,\n",
       " 11.85,\n",
       " 1.63,\n",
       " 1.32,\n",
       " 0.35,\n",
       " 1.26,\n",
       " 1.07,\n",
       " 3.34,\n",
       " 0.75,\n",
       " 1.01,\n",
       " 0.92,\n",
       " 2.18,\n",
       " 1.28,\n",
       " 0.87,\n",
       " 0.4,\n",
       " 2.19,\n",
       " 0.57,\n",
       " 1.38,\n",
       " 0.87,\n",
       " 0.68,\n",
       " 1.26,\n",
       " 1.91,\n",
       " 1.65,\n",
       " 0.42,\n",
       " 3.34,\n",
       " 3.36,\n",
       " 0.68,\n",
       " 0.44,\n",
       " 13.53,\n",
       " 0.4,\n",
       " 12.83,\n",
       " 0.75,\n",
       " 0.3,\n",
       " 6.73,\n",
       " 0.84,\n",
       " 1.27,\n",
       " 1.75,\n",
       " 1.28,\n",
       " 2.43,\n",
       " 3.28,\n",
       " 1.43,\n",
       " 2.29,\n",
       " 0.54,\n",
       " 1.1,\n",
       " 0.51,\n",
       " 0.7,\n",
       " 0.66,\n",
       " 0.44,\n",
       " 2.51,\n",
       " 3.21,\n",
       " 0.44,\n",
       " 0.65,\n",
       " 1.16,\n",
       " 1.44,\n",
       " 0.82,\n",
       " 2.74,\n",
       " 1.58,\n",
       " 1.71,\n",
       " 0.26,\n",
       " 0.42,\n",
       " 0.26,\n",
       " 1.28,\n",
       " 6.0,\n",
       " 0.25,\n",
       " 0.92,\n",
       " 0.37,\n",
       " 0.46,\n",
       " 0.61,\n",
       " 0.72,\n",
       " 4.56,\n",
       " 3.02,\n",
       " 1.46,\n",
       " 0.21,\n",
       " 1.13,\n",
       " 0.61,\n",
       " 2.07,\n",
       " 1.48,\n",
       " 1.0,\n",
       " 3.23,\n",
       " 0.52,\n",
       " 0.37,\n",
       " 1.08,\n",
       " 0.88,\n",
       " 0.72,\n",
       " 1.32,\n",
       " 0.5,\n",
       " 1.06,\n",
       " 2.74,\n",
       " 1.53,\n",
       " 3.36,\n",
       " 1.24,\n",
       " 1.94,\n",
       " 2.03,\n",
       " 0.56,\n",
       " 0.53,\n",
       " 0.44,\n",
       " 0.89,\n",
       " 2.28,\n",
       " 0.79,\n",
       " 0.82,\n",
       " 2.18,\n",
       " 1.56,\n",
       " 1.6,\n",
       " 0.67,\n",
       " 2.79,\n",
       " 0.57,\n",
       " 0.39,\n",
       " 1.47,\n",
       " 0.84,\n",
       " 0.73,\n",
       " 1.6,\n",
       " 1.01,\n",
       " 0.24,\n",
       " 0.51,\n",
       " 0.47,\n",
       " 1.75,\n",
       " 0.55,\n",
       " 2.48,\n",
       " 2.11,\n",
       " 1.65,\n",
       " 0.61,\n",
       " 2.33,\n",
       " 1.71,\n",
       " 1.79,\n",
       " 0.7,\n",
       " 1.31,\n",
       " 1.0,\n",
       " 0.84,\n",
       " 0.53,\n",
       " 1.19,\n",
       " 2.97,\n",
       " 0.91,\n",
       " 2.74,\n",
       " 0.47,\n",
       " 0.66,\n",
       " 1.08,\n",
       " 1.02,\n",
       " 1.22,\n",
       " 2.09,\n",
       " 0.97,\n",
       " 0.71,\n",
       " 1.34,\n",
       " 1.09,\n",
       " 3.25,\n",
       " 1.11,\n",
       " 0.66,\n",
       " 0.38,\n",
       " 0.94,\n",
       " 0.87,\n",
       " 0.49,\n",
       " 13.2,\n",
       " 0.35,\n",
       " 1.0,\n",
       " 3.05,\n",
       " 0.93,\n",
       " 0.22,\n",
       " 1.17,\n",
       " 1.09,\n",
       " 1.13,\n",
       " 0.81,\n",
       " 0.41,\n",
       " 3.79,\n",
       " 1.95,\n",
       " 0.41,\n",
       " 0.18,\n",
       " 1.15,\n",
       " 1.37,\n",
       " 1.96,\n",
       " 0.74,\n",
       " 1.3,\n",
       " 1.89,\n",
       " 0.78,\n",
       " 1.07,\n",
       " 1.5,\n",
       " 0.78,\n",
       " 1.97,\n",
       " 0.61,\n",
       " 1.7,\n",
       " 5.76,\n",
       " 2.56,\n",
       " 0.77,\n",
       " 0.49,\n",
       " 2.17,\n",
       " 0.74,\n",
       " 0.46,\n",
       " 1.11,\n",
       " 0.69,\n",
       " 0.64,\n",
       " 2.24,\n",
       " 2.05,\n",
       " 1.65,\n",
       " 0.84,\n",
       " 0.63,\n",
       " 0.57,\n",
       " 9.86,\n",
       " 18.9,\n",
       " 0.34,\n",
       " 4.57,\n",
       " 0.33,\n",
       " 2.35,\n",
       " 1.88,\n",
       " 2.55,\n",
       " 4.41,\n",
       " 1.21,\n",
       " 0.68,\n",
       " 0.81,\n",
       " 1.46,\n",
       " 0.46,\n",
       " 1.08,\n",
       " 0.5,\n",
       " 0.79,\n",
       " 0.55,\n",
       " 1.42,\n",
       " 4.56,\n",
       " 0.25,\n",
       " 1.87,\n",
       " 1.03,\n",
       " 1.03,\n",
       " 1.82,\n",
       " 0.43,\n",
       " 1.9,\n",
       " 0.3,\n",
       " 0.22,\n",
       " 0.4,\n",
       " 0.45,\n",
       " 1.28,\n",
       " 1.76,\n",
       " 0.62,\n",
       " 1.41,\n",
       " 0.51,\n",
       " 0.6,\n",
       " 0.92,\n",
       " 0.92,\n",
       " 2.75,\n",
       " 1.02,\n",
       " 1.13,\n",
       " 0.57,\n",
       " 0.65,\n",
       " 0.81,\n",
       " 2.65,\n",
       " 1.14,\n",
       " 5.53,\n",
       " 2.28,\n",
       " 1.12,\n",
       " 2.0,\n",
       " 0.46,\n",
       " 0.69,\n",
       " 1.58,\n",
       " 0.59,\n",
       " 3.43,\n",
       " 0.8,\n",
       " 1.29,\n",
       " 10.48,\n",
       " 0.99,\n",
       " 1.04,\n",
       " 0.55,\n",
       " 0.53,\n",
       " 0.79,\n",
       " 1.44,\n",
       " 1.42,\n",
       " 2.31,\n",
       " 0.4,\n",
       " 1.21,\n",
       " 1.15,\n",
       " 0.59,\n",
       " 1.38,\n",
       " 1.46,\n",
       " 0.69,\n",
       " 0.43,\n",
       " 2.01,\n",
       " 0.91,\n",
       " 0.94,\n",
       " 0.44,\n",
       " 0.33,\n",
       " 0.75,\n",
       " 1.8,\n",
       " 2.86,\n",
       " 0.47,\n",
       " 2.08,\n",
       " 0.95,\n",
       " 0.65,\n",
       " 0.95,\n",
       " 0.5,\n",
       " 3.82,\n",
       " 0.9,\n",
       " 7.64,\n",
       " 1.53,\n",
       " 1.52,\n",
       " 0.5,\n",
       " 1.03,\n",
       " 0.6,\n",
       " 1.66,\n",
       " 1.21,\n",
       " 0.22,\n",
       " 2.06,\n",
       " 0.45,\n",
       " 2.08,\n",
       " 0.36,\n",
       " 1.09,\n",
       " 1.73,\n",
       " 1.23,\n",
       " 0.47,\n",
       " 2.07,\n",
       " 0.3,\n",
       " 0.85,\n",
       " 2.09,\n",
       " 1.28,\n",
       " 0.91,\n",
       " 0.5,\n",
       " 1.11,\n",
       " 0.27,\n",
       " 0.3,\n",
       " 3.03,\n",
       " 0.85,\n",
       " 0.49,\n",
       " 0.53,\n",
       " 0.22,\n",
       " 0.45,\n",
       " 0.87,\n",
       " 1.86,\n",
       " 0.26,\n",
       " 1.78,\n",
       " 0.59,\n",
       " 0.55,\n",
       " 0.78,\n",
       " 1.54,\n",
       " 2.03,\n",
       " 0.73,\n",
       " 0.51,\n",
       " 0.63,\n",
       " 4.53,\n",
       " 0.63,\n",
       " 1.73,\n",
       " 0.87,\n",
       " 0.75,\n",
       " 1.28,\n",
       " 0.58,\n",
       " 0.59,\n",
       " 4.73,\n",
       " 1.12,\n",
       " 2.79,\n",
       " 0.98,\n",
       " 1.1,\n",
       " 3.81,\n",
       " 0.79,\n",
       " 0.6,\n",
       " 0.64,\n",
       " 1.73,\n",
       " 34.09,\n",
       " 0.79,\n",
       " 0.53,\n",
       " 0.2,\n",
       " 0.92,\n",
       " 1.05,\n",
       " 6.12,\n",
       " 0.65,\n",
       " 0.4,\n",
       " 5.06,\n",
       " 0.4,\n",
       " 1.88,\n",
       " 2.85,\n",
       " 0.36,\n",
       " 1.53,\n",
       " 4.16,\n",
       " 1.09,\n",
       " 1.14,\n",
       " 5.11,\n",
       " 0.45,\n",
       " 0.75,\n",
       " 0.56,\n",
       " 0.62,\n",
       " 4.4,\n",
       " 0.89,\n",
       " 1.06,\n",
       " 0.38,\n",
       " 6.31,\n",
       " 0.58,\n",
       " 0.49,\n",
       " 3.68,\n",
       " 1.1,\n",
       " 1.3,\n",
       " 0.61,\n",
       " 3.84,\n",
       " 0.93,\n",
       " 1.41,\n",
       " 1.5,\n",
       " 1.45,\n",
       " 1.61,\n",
       " 0.58,\n",
       " 1.54,\n",
       " 7.98,\n",
       " 1.25,\n",
       " 1.48,\n",
       " 1.63,\n",
       " 1.0,\n",
       " 1.02,\n",
       " 0.38,\n",
       " 0.91,\n",
       " 2.78,\n",
       " 0.71,\n",
       " 1.78,\n",
       " 1.11,\n",
       " 0.64,\n",
       " 1.03,\n",
       " 0.52,\n",
       " 1.39,\n",
       " 0.56,\n",
       " 0.39,\n",
       " 0.66,\n",
       " 0.94,\n",
       " 1.01,\n",
       " 0.17,\n",
       " 0.88,\n",
       " 0.88,\n",
       " 1.82,\n",
       " 0.76,\n",
       " 0.34,\n",
       " 0.45,\n",
       " 1.16,\n",
       " 0.48,\n",
       " 0.97,\n",
       " 2.3,\n",
       " 0.3,\n",
       " 1.71,\n",
       " 0.74,\n",
       " 1.73,\n",
       " 3.0,\n",
       " 0.54,\n",
       " 0.81,\n",
       " 0.77,\n",
       " 0.58,\n",
       " 2.25,\n",
       " 0.51,\n",
       " 5.12,\n",
       " 0.61,\n",
       " 0.53,\n",
       " 1.51,\n",
       " 1.07,\n",
       " 1.07,\n",
       " 1.99,\n",
       " 0.99,\n",
       " 4.99,\n",
       " 1.53,\n",
       " 0.76,\n",
       " 1.48,\n",
       " 4.29,\n",
       " 0.6,\n",
       " 0.61,\n",
       " 0.59,\n",
       " 0.95,\n",
       " 0.44,\n",
       " 0.21,\n",
       " 3.74,\n",
       " 0.93,\n",
       " 0.29,\n",
       " 4.72,\n",
       " 1.33,\n",
       " ...]"
      ]
     },
     "execution_count": 538,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 537,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x150b9ca10>"
      ]
     },
     "execution_count": 537,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "pd.DataFrame({'loss':loss_list},index=np.arange(len(loss_list))).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rough"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 453,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[293.9300,   0.0000,   0.0000,   0.0000,   0.0000, 122.6000,   0.0000,\n",
      "          64.8600,   0.0000,   0.0000],\n",
      "        [525.8300,   0.0000,   0.0000,   0.0000,   0.0000,  98.9300,   0.0000,\n",
      "          30.8900,   0.0000,   0.0000]])\n",
      "tensor([[344.9200,   0.0000,   0.0000,   0.0000,   0.0000]])\n",
      "[[Timestamp('2019-10-01 00:00:00') 'megadriving0b-20']\n",
      " [Timestamp('2019-11-01 00:00:00') 'megadriving0b-20']]\n"
     ]
    }
   ],
   "source": [
    "iterator=iter(dataloader)\n",
    "feature, target,index=iterator.next()\n",
    "i=15\n",
    "print(feature[i,])\n",
    "print(target[i,])\n",
    "print(non_feature_array[index[i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 456,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>gl_top_30</th>\n",
       "      <th>-999</th>\n",
       "      <th>121.0</th>\n",
       "      <th>201.0</th>\n",
       "      <th>147.0</th>\n",
       "      <th>200.0</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>store_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>megadriving0b-20</th>\n",
       "      <td>293.93</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "gl_top_30           -999  121.0  201.0  147.0  200.0\n",
       "store_id                                            \n",
       "megadriving0b-20  293.93    0.0    0.0    0.0    0.0"
      ]
     },
     "execution_count": 456,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sample_1.query('store_id==\"megadriving0b-20\"')['Direct_OPS'].swaplevel(1,0,axis=1)['2019-10-01']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 457,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>gl_top_30</th>\n",
       "      <th>-999</th>\n",
       "      <th>121.0</th>\n",
       "      <th>201.0</th>\n",
       "      <th>147.0</th>\n",
       "      <th>200.0</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>store_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>megadriving0b-20</th>\n",
       "      <td>122.6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>64.86</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "gl_top_30          -999  121.0  201.0  147.0  200.0\n",
       "store_id                                           \n",
       "megadriving0b-20  122.6    0.0    0.0  64.86    0.0"
      ]
     },
     "execution_count": 457,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sample_1.query('store_id==\"megadriving0b-20\"')['Halo_OPS'].swaplevel(1,0,axis=1)['2019-10-01']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 458,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Direct_OPS_-999', 'Direct_OPS_121.0', 'Direct_OPS_147.0',\n",
       "       'Direct_OPS_200.0', 'Direct_OPS_201.0', 'Halo_OPS_-999',\n",
       "       'Halo_OPS_121.0', 'Halo_OPS_147.0', 'Halo_OPS_200.0', 'Halo_OPS_201.0'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 458,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 475,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2815.2600,    0.0000,    0.0000,    0.0000,    0.0000,  644.3900,\n",
      "           80.4200,   11.9900,   10.9800,   14.9800],\n",
      "        [1381.0300,    0.0000,    0.0000,    0.0000,    0.0000,  756.5500,\n",
      "            0.0000,    0.0000,    0.0000,    0.0000]])\n",
      "tensor([[2290.5701,   19.8800,    0.0000,   23.9900,  170.3900]])\n",
      "[[Timestamp('2020-01-01 00:00:00') 'kemesab-20']\n",
      " [Timestamp('2020-02-01 00:00:00') 'kemesab-20']]\n"
     ]
    }
   ],
   "source": [
    "i=0\n",
    "print(feature[i,])\n",
    "print(target[i,])\n",
    "print(non_feature_array[index[i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 470,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Direct_OPS_-999', 'Direct_OPS_121.0', 'Direct_OPS_147.0',\n",
       "       'Direct_OPS_200.0', 'Direct_OPS_201.0', 'Halo_OPS_-999',\n",
       "       'Halo_OPS_121.0', 'Halo_OPS_147.0', 'Halo_OPS_200.0', 'Halo_OPS_201.0'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 470,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 477,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>gl_top_30</th>\n",
       "      <th>-999</th>\n",
       "      <th>121.0</th>\n",
       "      <th>201.0</th>\n",
       "      <th>200.0</th>\n",
       "      <th>147.0</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>store_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>kemesab-20</th>\n",
       "      <td>644.39</td>\n",
       "      <td>80.42</td>\n",
       "      <td>14.98</td>\n",
       "      <td>10.98</td>\n",
       "      <td>11.99</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "gl_top_30     -999  121.0  201.0  200.0  147.0\n",
       "store_id                                      \n",
       "kemesab-20  644.39  80.42  14.98  10.98  11.99"
      ]
     },
     "execution_count": 477,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sample_1.query('store_id==\"kemesab-20\"')['Halo_OPS'].swaplevel(1,0,axis=1)['2020-01-01']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_dataloader=DataLoader(train_dataset, batch_size=30)\n",
    "# test_dataloader=DataLoader(test_dataset, batch_size=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([30, 2, 10])"
      ]
     },
     "execution_count": 403,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
